input {
  # Input for data acquisition logs.
  file {
    path => "/Users/prabhatchanda/PycharmProjects/Amazon-Reviews-Sentiment-Analysis/data_pipeline/logs/scheduler/latest/data_acquisition_dag.py.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "json"
    type => "acquisition"  # Adding a type to differentiate between inputs
  }

  # Input for data sampling logs
  file {
    path => "/Users/prabhatchanda/PycharmProjects/Amazon-Reviews-Sentiment-Analysis/data_pipeline/logs/scheduler/latest/sampling_dag.py.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "json"
    type => "sampling"  # Adding a different type for drift logs
  }

  # Input for data validation logs
  file {
    path => "/Users/prabhatchanda/PycharmProjects/Amazon-Reviews-Sentiment-Analysis/data_pipeline/logs/scheduler/latest/data_validation_dag.py.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "json"
    type => "validation"  # Adding a different type for drift logs
  }

  # Input for data preprocessing logs
  file {
    path => "/Users/prabhatchanda/PycharmProjects/Amazon-Reviews-Sentiment-Analysis/data_pipeline/logs/scheduler/latest/data_preprocessing_dag.py.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "json"
    type => "preprocessing"  # Adding a different type for drift logs
  }
}

filter {
  grok {
    match => {
      "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] \{%{DATA:component}\} %{LOGLEVEL:log_level} - (?:\[%{TIMESTAMP_ISO8601:inner_timestamp}\] \{%{DATA:inner_component}\} %{LOGLEVEL:inner_log_level} - )?%{GREEDYDATA:log_message}"
    }
  }

  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }

  if [log_message] =~ "DAG\(s\) '.*' retrieved from" {
    grok {
      match => {
        "log_message" => "DAG\(s\) '%{DATA:dag_name}' retrieved from %{GREEDYDATA:dag_file_path}"
      }
    }
  }

  if [log_message] =~ "Started process \(PID=\d+\) to work on" {
    grok {
      match => {
        "log_message" => "Started process \(PID=%{NUMBER:pid}\) to work on %{GREEDYDATA:dag_file_path}"
      }
    }
  }

  if [log_message] =~ "Processing file .* for tasks to queue" {
    grok {
      match => {
        "log_message" => "Processing file %{GREEDYDATA:dag_file_path} for tasks to queue"
      }
    }
  }

  if [log_message] =~ "Downloading required NLTK packages" {
    grok {
      match => {
        "log_message" => "Downloading required NLTK packages..."
      }
    }
    mutate {
      add_field => { "nltk_download" => "true" }
    }
  }

  if [log_message] =~ "Defining aspects and keywords" {
    grok {
      match => {
        "log_message" => "Defining aspects and keywords..."
      }
    }
    mutate {
      add_field => { "aspect_definition" => "true" }
    }
  }

  mutate {
    remove_field => [ "message" ]
  }
}

output {
  # Output to Elasticsearch index based on log type
  if [type] == "acquisition" {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "logstashdataacquisitiondaglog"
    }
    stdout {
      codec => rubydebug {
        metadata => false
      }
    }
  }

  if [type] == "sampling" {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "logstashdatasamplingdag"
    }
    stdout {
      codec => rubydebug {
        metadata => false
      }
    }
  }

  if [type] == "validation" {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "logstashdatavalidationdag"
    }
    stdout {
      codec => rubydebug {
        metadata => false
      }
    }
  }

  if [type] == "preprocessing" {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "logstashdatapreprocessingdag"
    }
    stdout {
      codec => rubydebug {
        metadata => false
      }
    }
  }
}