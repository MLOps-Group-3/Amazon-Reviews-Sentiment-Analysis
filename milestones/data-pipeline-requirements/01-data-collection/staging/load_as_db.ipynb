{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='data_processing.log', level=logging.ERROR, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def filter_data(input_file, db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS reviews (\n",
    "        rating REAL,\n",
    "        title TEXT,\n",
    "        text TEXT,\n",
    "        asin TEXT,\n",
    "        parent_asin TEXT,\n",
    "        user_id TEXT,\n",
    "        timestamp INTEGER,\n",
    "        helpful_vote INTEGER,\n",
    "        verified_purchase BOOLEAN\n",
    "    )\n",
    "    ''')\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Check if the timestamp is after 2018 (in milliseconds)\n",
    "                if data['timestamp'] >= 1514764800000:  # 1514764800000 is Jan 1, 2018 in Unix ms\n",
    "                    # Insert filtered data into the SQL table\n",
    "                    cursor.execute('''\n",
    "                    INSERT INTO reviews (rating, title, text, asin, parent_asin, user_id, timestamp, helpful_vote, verified_purchase)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', (\n",
    "                        data['rating'], data['title'], data['text'],\n",
    "                        data['asin'], data['parent_asin'], data['user_id'],\n",
    "                        data['timestamp'], data['helpful_vote'],\n",
    "                        data['verified_purchase']\n",
    "                    ))\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"JSONDecodeError on line {line_number}: {str(e)}\")\n",
    "            except sqlite3.Error as e:\n",
    "                logging.error(f\"SQLite error on line {line_number}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error on line {line_number}: {str(e)}\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data('Home_and_Kitchen.jsonl', 'Home_and_Kitchen_reviews.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(filename='data_processing.log', level=logging.ERROR,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def save_matching_data_to_db(input_file, db_name, filtered_db):\n",
    "    # Connect to the filtered database to get the list of parent_asin values\n",
    "    filtered_conn = sqlite3.connect(filtered_db)\n",
    "    filtered_cursor = filtered_conn.cursor()\n",
    "    \n",
    "    # Get distinct parent_asin from the filtered database\n",
    "    filtered_cursor.execute(\"SELECT DISTINCT parent_asin FROM reviews\")\n",
    "    parent_asins = set(row[0] for row in filtered_cursor.fetchall())\n",
    "    filtered_conn.close()\n",
    "\n",
    "    # Connect to the target database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create a table for the product details\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS products (\n",
    "        main_category TEXT,\n",
    "        title TEXT,\n",
    "        average_rating REAL,\n",
    "        rating_number INTEGER,\n",
    "        features TEXT,\n",
    "        description TEXT,\n",
    "        price REAL,\n",
    "        store TEXT,\n",
    "        categories TEXT,\n",
    "        brand TEXT,\n",
    "        material TEXT,\n",
    "        color TEXT,\n",
    "        capacity TEXT,\n",
    "        style TEXT,\n",
    "        pattern TEXT,\n",
    "        care_instructions TEXT,\n",
    "        unit_count TEXT,\n",
    "        dimensions TEXT,\n",
    "        num_items INTEGER,\n",
    "        item_weight TEXT,\n",
    "        best_sellers_rank TEXT,\n",
    "        discontinued TEXT,\n",
    "        date_first_available TEXT,\n",
    "        parent_asin TEXT\n",
    "    )\n",
    "    ''')\n",
    "\n",
    "    # Open the large dataset file\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            try:\n",
    "                # Parse each line as JSON\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Check if the parent_asin matches the filtered set\n",
    "                parent_asin = data.get('parent_asin', None)\n",
    "                if parent_asin in parent_asins:\n",
    "                    # Extract details from the JSON\n",
    "                    main_category = data.get('main_category', None)\n",
    "                    title = data.get('title', None)\n",
    "                    average_rating = data.get('average_rating', None)\n",
    "                    rating_number = data.get('rating_number', None)\n",
    "                    features = json.dumps(data.get('features', []))\n",
    "                    description = json.dumps(data.get('description', []))\n",
    "                    price = data.get('price', None)\n",
    "                    store = data.get('store', None)\n",
    "                    categories = json.dumps(data.get('categories', []))\n",
    "                    details = data.get('details', {})\n",
    "                    brand = details.get('Brand', None)\n",
    "                    material = details.get('Material', None)\n",
    "                    color = details.get('Color', None)\n",
    "                    capacity = details.get('Capacity', None)\n",
    "                    style = details.get('Style', None)\n",
    "                    pattern = details.get('Pattern', None)\n",
    "                    care_instructions = details.get('Product Care Instructions', None)\n",
    "                    unit_count = details.get('Unit Count', None)\n",
    "                    dimensions = details.get('Product Dimensions', None)\n",
    "                    num_items = details.get('Number of Items', None)\n",
    "                    item_weight = details.get('Item Weight', None)\n",
    "                    best_sellers_rank = json.dumps(details.get('Best Sellers Rank', {}))\n",
    "                    discontinued = details.get('Is Discontinued By Manufacturer', None)\n",
    "                    date_first_available = details.get('Date First Available', None)\n",
    "\n",
    "                    # Insert the data into the SQL table\n",
    "                    cursor.execute('''\n",
    "                    INSERT INTO products (\n",
    "                        main_category, title, average_rating, rating_number, features,\n",
    "                        description, price, store, categories, brand, material, color,\n",
    "                        capacity, style, pattern, care_instructions, unit_count, dimensions,\n",
    "                        num_items, item_weight, best_sellers_rank, discontinued, date_first_available,\n",
    "                        parent_asin\n",
    "                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', (\n",
    "                        main_category, title, average_rating, rating_number, features,\n",
    "                        description, price, store, categories, brand, material, color,\n",
    "                        capacity, style, pattern, care_instructions, unit_count, dimensions,\n",
    "                        num_items, item_weight, best_sellers_rank, discontinued, date_first_available,\n",
    "                        parent_asin\n",
    "                    ))\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"JSONDecodeError on line {line_number}: {str(e)}\")\n",
    "            except sqlite3.Error as e:\n",
    "                logging.error(f\"SQLite error on line {line_number}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error on line {line_number}: {str(e)}\")\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "save_matching_data_to_db('meta_Home_and_Kitchen.jsonl', 'meta_Home_and_Kitchen.db', 'Home_and_Kitchen_reviews.db')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def query_sample_rows(db_name, table_name, num_rows=5):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Query the specified number of rows from the table\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT {num_rows}\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch the data and column names\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Convert to DataFrame for better readability\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # print(df)\n",
    "    return df\n",
    "\n",
    "# Usage example\n",
    "meta_df = query_sample_rows('meta_Home_and_Kitchen.db', 'products', num_rows=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df#.columns#['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(query,db_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Query the specified number of rows from the table\n",
    "    # query = f\"SELECT * FROM {table_name} LIMIT {num_rows}\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch the data and column names\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Convert to DataFrame for better readability\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # print(df)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT \n",
    "            *\n",
    "            FROM products\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "query_db(query,'meta_Home_and_Kitchen.db').to_csv('meta_Home_and_Kitchen_sample.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT \n",
    "            *\n",
    "            FROM reviews\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "query_db(query,'Home_and_Kitchen_reviews.db').to_csv('reviews_Home_and_Kitchen_sample.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "query = \"\"\"SELECT \n",
    "            DISTINCT main_category, \n",
    "            COUNT(parent_asin) AS product_count\n",
    "            FROM products\n",
    "            GROUP BY(main_category)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "meta_df = query_db(query,'meta_Home_and_Kitchen.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT \n",
    "            *\n",
    "            FROM products\n",
    "            WHERE main_category=\"Health & Personal Care\"\n",
    "            -- GROUP BY(main_category)\n",
    "        \"\"\"\n",
    "temp_df = query_db(query,'meta_Home_and_Kitchen.db')\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(temp_df[\"store\"][:].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT \n",
    "            DISTINCT store,\n",
    "            COUNT(parent_asin) AS product_count\n",
    "            FROM products\n",
    "            WHERE main_category=\"Amazon Home\"\n",
    "            GROUP BY(store)\n",
    "            ORDER BY product_count DESC\n",
    "            LIMIT 100\n",
    "        \"\"\"\n",
    "query_db(query,'meta_Home_and_Kitchen.db')\n",
    "# temp_df.to_csv(\"store_product_count_from_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT *\n",
    "            FROM products\n",
    "            WHERE title LIKE \"%office desk,%\" \n",
    "        \"\"\"\n",
    "temp_df = query_db(query,'meta_Home_and_Kitchen.db')\n",
    "# temp_df.to_csv(\"store_product_count_from_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['features'].loc[0]#0['style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT *\n",
    "            FROM reviews\n",
    "    WHERE parent_asin=\"B07TP32NF7\"\n",
    "    \"\"\"\n",
    "query_db(query,'Home_and_Kitchen_reviews.db')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "def query_db(db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('SELECT MIN(timestamp), MAX(timestamp) FROM reviews')\n",
    "    min_max_result = cursor.fetchone()\n",
    "    \n",
    "    if min_max_result[0] is not None and min_max_result[1] is not None:\n",
    "        min_date = datetime.fromtimestamp(min_max_result[0] / 1000)\n",
    "        max_date = datetime.fromtimestamp(min_max_result[1] / 1000)\n",
    "        print(f\"Minimum Timestamp: {min_max_result[0]} ({min_date})\")\n",
    "        print(f\"Maximum Timestamp: {min_max_result[1]} ({max_date})\")\n",
    "    else:\n",
    "        print(\"No data found in the database.\")\n",
    "\n",
    "    cursor.execute('SELECT COUNT(*) FROM reviews')\n",
    "    total_rows = cursor.fetchone()[0]\n",
    "    print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "query_db('Home_and_Kitchen_reviews.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def query_aggregated_data(db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    WITH Aggregated AS (\n",
    "        SELECT\n",
    "            parent_asin,\n",
    "            MIN(timestamp) AS review_start,\n",
    "            MAX(timestamp) AS review_end,\n",
    "            COUNT(timestamp) AS review_count\n",
    "        FROM reviews\n",
    "        GROUP BY parent_asin\n",
    "    ),\n",
    "    DateComponents AS (\n",
    "        SELECT\n",
    "            parent_asin,\n",
    "            review_start,\n",
    "            review_end,\n",
    "            review_count,\n",
    "            strftime('%Y', datetime(review_start / 1000, 'unixepoch')) AS review_start_year,\n",
    "            strftime('%Y', datetime(review_end / 1000, 'unixepoch')) AS review_end_year,\n",
    "            strftime('%m', datetime(review_start / 1000, 'unixepoch')) AS review_start_month,\n",
    "            strftime('%m', datetime(review_end / 1000, 'unixepoch')) AS review_end_month,\n",
    "            date(datetime(review_start / 1000, 'unixepoch')) AS review_start_date,\n",
    "            date(datetime(review_end / 1000, 'unixepoch')) AS review_end_date\n",
    "        FROM Aggregated\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM DateComponents\n",
    "    WHERE \n",
    "        review_start_year >= '2018' AND\n",
    "        review_start_year <= '2020' AND\n",
    "        review_end_year > '2020' AND\n",
    "        review_count > 100\n",
    "    ORDER BY review_end_date DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    aggregated_data = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    print(\"Total reviews:\", aggregated_data['review_count'].sum())\n",
    "    return aggregated_data\n",
    "\n",
    "aggregated_data_sql_df = query_aggregated_data('Home_and_Kitchen_reviews.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_sql_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def query_aggregated_data(db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    \n",
    "    query = \"\"\"\n",
    "WITH Aggregated AS (\n",
    "    SELECT\n",
    "        parent_asin,\n",
    "        MIN(timestamp) AS review_start,\n",
    "        MAX(timestamp) AS review_end,\n",
    "        COUNT(timestamp) AS review_count\n",
    "    FROM reviews\n",
    "    GROUP BY parent_asin\n",
    "),\n",
    "DateComponents AS (\n",
    "    SELECT\n",
    "        parent_asin,\n",
    "        review_start,\n",
    "        review_end,\n",
    "        review_count,\n",
    "        strftime('%Y', datetime(review_start / 1000, 'unixepoch')) AS review_start_year,\n",
    "        strftime('%Y', datetime(review_end / 1000, 'unixepoch')) AS review_end_year,\n",
    "        strftime('%m', datetime(review_start / 1000, 'unixepoch')) AS review_start_month,\n",
    "        strftime('%m', datetime(review_end / 1000, 'unixepoch')) AS review_end_month,\n",
    "        date(datetime(review_start / 1000, 'unixepoch')) AS review_start_date,\n",
    "        date(datetime(review_end / 1000, 'unixepoch')) AS review_end_date\n",
    "    FROM Aggregated\n",
    "    WHERE \n",
    "        review_start_year >= '2018' AND\n",
    "        review_start_year <= '2020' AND\n",
    "        review_end_year > '2020' AND\n",
    "        review_count > 100\n",
    "),\n",
    "RatingCounts AS (\n",
    "    SELECT \n",
    "        asin,\n",
    "        parent_asin,\n",
    "        COUNT(CASE WHEN rating = 1 THEN 1 END) AS rating_1_count,\n",
    "        COUNT(CASE WHEN rating = 2 THEN 1 END) AS rating_2_count,\n",
    "        COUNT(CASE WHEN rating = 3 THEN 1 END) AS rating_3_count,\n",
    "        COUNT(CASE WHEN rating = 4 THEN 1 END) AS rating_4_count,\n",
    "        COUNT(CASE WHEN rating = 5 THEN 1 END) AS rating_5_count,\n",
    "        COUNT(asin) AS total_count\n",
    "    FROM reviews\n",
    "    GROUP BY asin, parent_asin\n",
    ")\n",
    "SELECT \n",
    "    d.parent_asin,\n",
    "    r.asin,\n",
    "    d.review_start,\n",
    "    d.review_end,\n",
    "    d.review_start_date,\n",
    "    d.review_end_date,\n",
    "    d.review_count,\n",
    "    r.rating_1_count,\n",
    "    r.rating_2_count,\n",
    "    r.rating_3_count,\n",
    "    r.rating_4_count,\n",
    "    r.rating_5_count,\n",
    "    r.total_count,\n",
    "    ROUND(CAST(r.rating_1_count AS FLOAT) / r.total_count * 100, 2) AS neg_review_percentage,\n",
    "    ROUND(CAST(r.rating_5_count AS FLOAT) / r.total_count * 100, 2) AS pos_review_percentage\n",
    "FROM \n",
    "    DateComponents d\n",
    "JOIN \n",
    "    RatingCounts r\n",
    "ON \n",
    "    d.parent_asin = r.parent_asin\n",
    "ORDER BY \n",
    "    d.review_end_date DESC;\n",
    "    \"\"\"\n",
    "    \n",
    "    aggregated_data = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    print(\"Total reviews:\", aggregated_data['review_count'].sum())\n",
    "    return aggregated_data\n",
    "\n",
    "aggregated_data_sql_df = query_aggregated_data('Home_and_Kitchen_reviews.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_sql_df.to_csv(\"Home_and_Kitchen_rating_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_distribution(db_name):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    \n",
    "    # SQL query to get rating distribution grouped by asin and rating\n",
    "    query = \"\"\"\n",
    "SELECT \n",
    "    parent_asin,asin,\n",
    "    COUNT(CASE WHEN rating = 1 THEN 1 END) AS rating_1_count,\n",
    "    COUNT(CASE WHEN rating = 2 THEN 1 END) AS rating_2_count,\n",
    "    COUNT(CASE WHEN rating = 3 THEN 1 END) AS rating_3_count,\n",
    "    COUNT(CASE WHEN rating = 4 THEN 1 END) AS rating_4_count,\n",
    "    COUNT(CASE WHEN rating = 5 THEN 1 END) AS rating_5_count,\n",
    "    COUNT(asin) AS total_count\n",
    "FROM \n",
    "    reviews\n",
    "GROUP BY \n",
    "    parent_asin,asin\n",
    "ORDER BY \n",
    "    asin;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query and load results into a DataFrame\n",
    "    rating_distribution = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    # Close connection\n",
    "    conn.close()\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # print(rating_distribution)\n",
    "    return rating_distribution\n",
    "\n",
    "rating_distribution = get_rating_distribution('Home_and_Kitchen_reviews.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "query = \"\"\"SELECT COUNT(*)\n",
    "            FROM reviews\n",
    "            -- LIMIT 20\n",
    "        \"\"\"\n",
    "query_db(query,'Home_and_Kitchen_reviews.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1514764800000\n",
    "h_k_5_core_df = pd.read_csv('Home_and_Kitchen_5_core.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_k_5_core_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_k_5_core_df = h_k_5_core_df[h_k_5_core_df['timestamp']>=1514764800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_k_5_core_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(h_k_5_core_df['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db_with_user_ids(user_ids, db_name):\n",
    "    # Convert the list of user_ids to a format suitable for SQL IN clause\n",
    "    user_ids_str = ','.join([f\"'{user_id}'\" for user_id in user_ids])\n",
    "    \n",
    "    # SQL query to select records where user_id is in the provided list\n",
    "    query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM reviews\n",
    "            WHERE user_id IN ({user_ids_str})\n",
    "            \"\"\"\n",
    "    # print(query)\n",
    "    # Call the existing query_db function to execute the query\n",
    "    return query_db(query, db_name)\n",
    "\n",
    "# Assuming you already have your DataFrame\n",
    "# user_ids = set(h_k_5_core_df['user_id'])#.tolist()\n",
    "\n",
    "# Query the database\n",
    "# result_df = query_db_with_user_ids(user_ids, 'Home_and_Kitchen_reviews.db')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = query_db_with_user_ids(user_ids, 'Home_and_Kitchen_reviews.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = h_k_5_core_df['user_id'].unique()#.tolist()\n",
    "\n",
    "temp = user_ids[:10]\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM reviews\n",
    "        WHERE user_id IN ({temp})\n",
    "        \"\"\"\n",
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_str = ','.join([f\"'{user_id}'\" for user_id in temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv('reviews_filtered_5core.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.groupby('parent_asin').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_review_distribution(df, pos_range=(5, 5), neg_range=(1, 1)):\n",
    "    # Group by 'parent_asin' and 'asin', then calculate rating counts (1.0 to 5.0)\n",
    "    asin_parent_group = df.groupby(['parent_asin', 'asin'])['rating'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Rename columns for clarity (rating_1, rating_2, ..., rating_5)\n",
    "    asin_parent_group.columns = [f'rating_{int(rating)}' for rating in asin_parent_group.columns]\n",
    "\n",
    "    # Calculate total ratings for each asin (sum of all rating columns)\n",
    "    asin_parent_group['total_ratings_asin'] = asin_parent_group.sum(axis=1)\n",
    "\n",
    "    # Calculate total positive and negative ratings based on the provided ranges\n",
    "    asin_parent_group['total_pos_asin'] = asin_parent_group[[f'rating_{i}' for i in range(pos_range[0], pos_range[1] + 1)]].sum(axis=1)\n",
    "    asin_parent_group['total_neg_asin'] = asin_parent_group[[f'rating_{i}' for i in range(neg_range[0], neg_range[1] + 1)]].sum(axis=1)\n",
    "\n",
    "    # Calculate positive and negative percentages for each asin\n",
    "    asin_parent_group['pos_per_asin'] = asin_parent_group['total_pos_asin'] / asin_parent_group['total_ratings_asin'] * 100\n",
    "    asin_parent_group['neg_per_asin'] = asin_parent_group['total_neg_asin'] / asin_parent_group['total_ratings_asin'] * 100\n",
    "\n",
    "    # Group by 'parent_asin' to calculate totals, including positive and negative ratings for each parent_asin\n",
    "    parent_group = asin_parent_group.groupby('parent_asin').sum()\n",
    "\n",
    "    # Calculate total ratings for the entire parent_asin (sum of all ratings for all asins under that parent_asin)\n",
    "    parent_group['total_ratings_parent_asin'] = parent_group[['rating_1', 'rating_2', 'rating_3', 'rating_4', 'rating_5']].sum(axis=1)\n",
    "\n",
    "    # Calculate total positive and negative ratings for each parent_asin\n",
    "    parent_group['total_pos_parent_asin'] = parent_group[[f'rating_{i}' for i in range(pos_range[0], pos_range[1] + 1)]].sum(axis=1)\n",
    "    parent_group['total_neg_parent_asin'] = parent_group[[f'rating_{i}' for i in range(neg_range[0], neg_range[1] + 1)]].sum(axis=1)\n",
    "\n",
    "    # Calculate positive and negative percentages for each parent_asin\n",
    "    parent_group['pos_per_parent_asin'] = parent_group['total_pos_parent_asin'] / parent_group['total_ratings_parent_asin'] * 100\n",
    "    parent_group['neg_per_parent_asin'] = parent_group['total_neg_parent_asin'] / parent_group['total_ratings_parent_asin'] * 100\n",
    "\n",
    "    # Merge asin-level and parent-level results for final display, including total ratings and totals for pos/neg\n",
    "    result = asin_parent_group.merge(parent_group[['total_ratings_parent_asin', 'pos_per_parent_asin', 'neg_per_parent_asin', 'total_pos_parent_asin', 'total_neg_parent_asin']],\n",
    "                                     left_on='parent_asin', right_index=True, how='left')\n",
    "\n",
    "    return result\n",
    "\n",
    "review_distribution = calculate_review_distribution(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_distribution = review_distribution.sort_values(by='total_ratings_parent_asin',ascending=False)#.get('parent_asin')\n",
    "sorted_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_distribution['total_neg_asin'].sum(),sorted_distribution['total_pos_asin'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_parent_asins = sorted_distribution.index.get_level_values('parent_asin').unique()#.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_parent_asins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_distribution\n",
    "\n",
    "review_distribution.loc[review_distribution.index.get_level_values('parent_asin')  == 'B08XYD548C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['parent_asin']=='B00U8QEXBS']#['verified_purchase'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
