# PIPELINE DEFINITION
# Name: sentiment-analysis-pipeline
# Description: A pipeline for sentiment analysis using a transformer model
# Inputs:
#    batch_size: int [Default: 32.0]
#    dataset_id: str
#    dropout_rate: float [Default: 0.1]
#    learning_rate: float [Default: 2e-05]
#    model_name: str [Default: 'bert-base-uncased']
#    num_epochs: int [Default: 3.0]
#    project_id: str
#    region: str
#    weight_decay: float [Default: 0.01]
# Outputs:
#    evaluate-model-metrics: system.Metrics
#    train-model-metrics: system.Metrics
components:
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        class_labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-prepare-data:
    executorLabel: exec-prepare-data
    inputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
        region:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        class_labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        class_labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        batch_size:
          parameterType: NUMBER_INTEGER
        dropout_rate:
          parameterType: NUMBER_DOUBLE
        learning_rate:
          parameterType: NUMBER_DOUBLE
        model_name:
          parameterType: STRING
        num_epochs:
          parameterType: NUMBER_INTEGER
        weight_decay:
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        trained_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas'\
          \ 'torch==1.12.1' 'transformers==4.21.0' 'scikit-learn' 'accelerate==0.12.0'\
          \ 'google-cloud-storage' 'kfp==2.0.0' 'PyYAML>=6.0' 'tensorboard' 'kfp==2.0.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(\n    model_path: Input[Model],\n    test_data:\
          \ Input[Dataset],\n    class_labels: Input[Dataset],\n    metrics: Output[Metrics],\n\
          ) -> None:\n    import pandas as pd\n    import torch\n    from transformers\
          \ import BertTokenizer, BertForSequenceClassification\n    from sklearn.metrics\
          \ import accuracy_score, precision_recall_fscore_support\n    import os\n\
          \n    # Determine device\n    device = torch.device(\"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\")\n    print(f\"Is CUDA available: {torch.cuda.is_available()}\"\
          )\n    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n    if\
          \ torch.cuda.is_available():\n        print(f\"Using CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\"\
          )\n    else:\n        print(\"No GPU found. Using CPU.\")\n\n\n    # Load\
          \ test data and class labels\n    test_df = pd.read_csv(test_data.path)\n\
          \    class_labels = pd.read_csv(class_labels.path, header=None).squeeze().tolist()\n\
          \n    # Load the model\n    tokenizer = BertTokenizer.from_pretrained(model_path.path)\n\
          \    model = BertForSequenceClassification.from_pretrained(model_path.path).to(device)\
          \  # Move model to GPU\n\n    class SentimentDataset(torch.utils.data.Dataset):\n\
          \        def __init__(self, texts, labels, tokenizer, max_length=128):\n\
          \            self.texts = texts\n            self.labels = labels\n    \
          \        self.tokenizer = tokenizer\n            self.max_length = max_length\n\
          \n        def __len__(self):\n            return len(self.texts)\n\n   \
          \     def __getitem__(self, idx):\n            text = self.texts[idx]\n\
          \            label = self.labels[idx]\n\n            encoding = self.tokenizer(\n\
          \                text,\n                padding=\"max_length\",\n      \
          \          truncation=True,\n                max_length=self.max_length,\n\
          \                return_tensors=\"pt\",\n            )\n            return\
          \ {\n                \"input_ids\": encoding[\"input_ids\"].flatten(),\n\
          \                \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n\
          \                \"labels\": torch.tensor(label, dtype=torch.long),\n  \
          \          }\n\n    test_dataset = SentimentDataset(test_df[\"text\"], test_df[\"\
          label\"], tokenizer)\n    model.eval()\n\n    predictions, true_labels =\
          \ [], []\n    with torch.no_grad():\n        for batch in torch.utils.data.DataLoader(test_dataset,\
          \ batch_size=16):\n            inputs = {\n                \"input_ids\"\
          : batch[\"input_ids\"].to(device),\n                \"attention_mask\":\
          \ batch[\"attention_mask\"].to(device),\n            }\n            outputs\
          \ = model(**inputs)\n            preds = outputs.logits.argmax(dim=1).cpu().numpy()\n\
          \            predictions.extend(preds)\n            true_labels.extend(batch[\"\
          labels\"].cpu().numpy())\n\n    precision, recall, f1, _ = precision_recall_fscore_support(true_labels,\
          \ predictions, average=\"weighted\")\n    accuracy = accuracy_score(true_labels,\
          \ predictions)\n\n    eval_results = {\n        \"accuracy\": accuracy,\n\
          \        \"precision\": precision,\n        \"recall\": recall,\n      \
          \  \"f1\": f1,\n    }\n\n    # Save evaluation metrics\n    os.makedirs(metrics.path,\
          \ exist_ok=True)\n    metrics_path = os.path.join(metrics.path, \"eval_metrics.json\"\
          )\n\n    import json\n\n    with open(metrics_path, \"w\") as f:\n     \
          \   json.dump(eval_results, f)\n\n    from google.cloud import storage\n\
          \n    # Upload metrics to GCS\n    storage_client = storage.Client()\n \
          \   bucket_name = metrics.path.split(\"/\")[2]\n    destination_blob_name\
          \ = \"/\".join(metrics.path.split(\"/\")[3:]) + \"/eval_metrics.json\"\n\
          \    blob = storage_client.bucket(bucket_name).blob(destination_blob_name)\n\
          \    blob.upload_from_filename(metrics_path)\n\n"
        image: us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-12.py310:latest
        resources:
          accelerator:
            count: '1'
            type: NVIDIA_TESLA_T4
          cpuLimit: 4.0
          memoryLimit: 16.0
    exec-prepare-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - prepare_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas'\
          \ 'scikit-learn' 'google-cloud-aiplatform' 'gcsfs' 'kfp==2.0.0' && \"$0\"\
          \ \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef prepare_data(\n    project_id: str,\n    region: str,\n    dataset_id:\
          \ str,\n    train_data: Output[Dataset],\n    val_data: Output[Dataset],\n\
          \    test_data: Output[Dataset],\n    class_labels: Output[Dataset],\n)\
          \ -> None:\n    from google.cloud import aiplatform\n    import pandas as\
          \ pd\n    from sklearn.preprocessing import LabelEncoder\n\n    # Initialize\
          \ Vertex AI\n    aiplatform.init(project=project_id, location=region)\n\n\
          \    # Load the dataset and retrieve the GCS URI\n    dataset = aiplatform.TabularDataset(dataset_id)\n\
          \    gcs_uri = dataset._gca_resource.metadata[\"inputConfig\"][\"gcsSource\"\
          ][\"uri\"][0]\n\n    # Load the data from GCS URI as a pandas DataFrame\n\
          \    df = pd.read_csv(gcs_uri)\n\n    # Data processing code\n    df['text']\
          \ = df['text'].fillna('')\n    df['title'] = df['title'].fillna('')\n  \
          \  df['price'] = pd.to_numeric(df['price'].replace(\"unknown\", None), errors='coerce')\n\
          \    df['price_missing'] = df['price'].isna().astype(int)\n    df['price']\
          \ = df['price'].fillna(0).astype(float)\n    df['helpful_vote'] = df['helpful_vote'].fillna(0).astype(int)\n\
          \    df['verified_purchase'] = df['verified_purchase'].apply(lambda x: 1\
          \ if x else 0)\n\n    label_encoder = LabelEncoder()\n    df['label'] =\
          \ label_encoder.fit_transform(df['sentiment_label'])\n\n    # Split the\
          \ data\n    train_end = int(len(df) * 0.8)\n    val_end = int(len(df) *\
          \ 0.9)\n\n    train_df = df.iloc[:train_end]\n    val_df = df.iloc[train_end:val_end]\n\
          \    test_df = df.iloc[val_end:]\n\n    train_df.to_csv(train_data.path,\
          \ index=False)\n    val_df.to_csv(val_data.path, index=False)\n    test_df.to_csv(test_data.path,\
          \ index=False)\n    pd.Series(label_encoder.classes_).to_csv(class_labels.path,\
          \ index=False, header=False)\n\n"
        image: python:3.9
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas'\
          \ 'torch==1.12.1' 'transformers==4.21.0' 'scikit-learn' 'accelerate==0.12.0'\
          \ 'google-cloud-storage' 'kfp==2.0.0' 'PyYAML>=6.0' 'tensorboard' 'kfp==2.0.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(\n    train_data: Input[Dataset],\n    val_data:\
          \ Input[Dataset],\n    class_labels: Input[Dataset],\n    model_name: str,\n\
          \    learning_rate: float,\n    batch_size: int,\n    num_epochs: int,\n\
          \    weight_decay: float,\n    dropout_rate: float,\n    trained_model:\
          \ Output[Model],\n    metrics: Output[Metrics],\n) -> None:\n    import\
          \ pandas as pd\n    import torch\n    from transformers import BertTokenizer,\
          \ BertForSequenceClassification, Trainer, TrainingArguments\n    from sklearn.metrics\
          \ import precision_recall_fscore_support, accuracy_score\n    import os\n\
          \n    # Determine device\n    device = torch.device(\"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\")\n    print(f\"Is CUDA available: {torch.cuda.is_available()}\"\
          )\n    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n    if\
          \ torch.cuda.is_available():\n        print(f\"Using CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\"\
          )\n    else:\n        print(\"No GPU found. Using CPU.\")\n\n    # Load\
          \ datasets\n    train_df = pd.read_csv(train_data.path)\n    val_df = pd.read_csv(val_data.path)\n\
          \    class_labels = pd.read_csv(class_labels.path, header=None).squeeze().tolist()\n\
          \n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    model =\
          \ BertForSequenceClassification.from_pretrained(\n        model_name, num_labels=len(class_labels)\n\
          \    ).to(device)  # Move model to GPU\n\n    class SentimentDataset(torch.utils.data.Dataset):\n\
          \        def __init__(self, texts, labels, tokenizer, max_length=128):\n\
          \            self.texts = texts\n            self.labels = labels\n    \
          \        self.tokenizer = tokenizer\n            self.max_length = max_length\n\
          \n        def __len__(self):\n            return len(self.texts)\n\n   \
          \     def __getitem__(self, idx):\n            text = self.texts[idx]\n\
          \            label = self.labels[idx]\n\n            encoding = self.tokenizer(\n\
          \                text,\n                padding=\"max_length\",\n      \
          \          truncation=True,\n                max_length=self.max_length,\n\
          \                return_tensors=\"pt\",\n            )\n            return\
          \ {\n                \"input_ids\": encoding[\"input_ids\"].flatten(),\n\
          \                \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n\
          \                \"labels\": torch.tensor(label, dtype=torch.long),\n  \
          \          }\n\n    train_dataset = SentimentDataset(train_df[\"text\"],\
          \ train_df[\"label\"], tokenizer)\n    val_dataset = SentimentDataset(val_df[\"\
          text\"], val_df[\"label\"], tokenizer)\n\n    training_args = TrainingArguments(\n\
          \        output_dir=\"./results\",\n        num_train_epochs=num_epochs,\n\
          \        per_device_train_batch_size=batch_size,\n        evaluation_strategy=\"\
          epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\"\
          ,\n        weight_decay=weight_decay,\n        load_best_model_at_end=True,\n\
          \    )\n\n    def compute_metrics(pred):\n        labels = pred.label_ids\n\
          \        preds = pred.predictions.argmax(-1)\n        precision, recall,\
          \ f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\"\
          )\n        accuracy = accuracy_score(labels, preds)\n        return {\"\
          accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\"\
          : f1}\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n\
          \        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n\
          \        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n\
          \n    tokenizer.save_pretrained(trained_model.path)\n    trainer.save_model(trained_model.path)\n\
          \n    # Ensure the output directory exists\n    os.makedirs(metrics.path,\
          \ exist_ok=True)\n\n    # Compute final metrics\n    eval_metrics = trainer.evaluate()\n\
          \    metrics_path = os.path.join(metrics.path, \"train_metrics.json\")\n\
          \n    import json \n\n    with open(metrics_path, \"w\") as f:\n       \
          \ json.dump(eval_metrics, f)\n\n    from google.cloud import storage\n\n\
          \    # Upload metrics to GCS\n    storage_client = storage.Client()\n  \
          \  bucket_name = metrics.path.split(\"/\")[2]\n    destination_blob_name\
          \ = \"/\".join(metrics.path.split(\"/\")[3:]) + \"/train_metrics.json\"\n\
          \    blob = storage_client.bucket(bucket_name).blob(destination_blob_name)\n\
          \    blob.upload_from_filename(metrics_path)\n\n"
        image: us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-12.py310:latest
        resources:
          accelerator:
            count: '1'
            type: NVIDIA_TESLA_T4
          cpuLimit: 8.0
          memoryLimit: 32.0
pipelineInfo:
  description: A pipeline for sentiment analysis using a transformer model
  name: sentiment-analysis-pipeline
root:
  dag:
    outputs:
      artifacts:
        evaluate-model-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: evaluate-model
        train-model-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: train-model
    tasks:
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - prepare-data
        - train-model
        inputs:
          artifacts:
            class_labels:
              taskOutputArtifact:
                outputArtifactKey: class_labels
                producerTask: prepare-data
            model_path:
              taskOutputArtifact:
                outputArtifactKey: trained_model
                producerTask: train-model
            test_data:
              taskOutputArtifact:
                outputArtifactKey: test_data
                producerTask: prepare-data
        taskInfo:
          name: evaluate-model
      prepare-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-prepare-data
        inputs:
          parameters:
            dataset_id:
              componentInputParameter: dataset_id
            project_id:
              componentInputParameter: project_id
            region:
              componentInputParameter: region
        taskInfo:
          name: prepare-data
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - prepare-data
        inputs:
          artifacts:
            class_labels:
              taskOutputArtifact:
                outputArtifactKey: class_labels
                producerTask: prepare-data
            train_data:
              taskOutputArtifact:
                outputArtifactKey: train_data
                producerTask: prepare-data
            val_data:
              taskOutputArtifact:
                outputArtifactKey: val_data
                producerTask: prepare-data
          parameters:
            batch_size:
              componentInputParameter: batch_size
            dropout_rate:
              componentInputParameter: dropout_rate
            learning_rate:
              componentInputParameter: learning_rate
            model_name:
              componentInputParameter: model_name
            num_epochs:
              componentInputParameter: num_epochs
            weight_decay:
              componentInputParameter: weight_decay
        taskInfo:
          name: train-model
  inputDefinitions:
    parameters:
      batch_size:
        defaultValue: 32.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      dataset_id:
        parameterType: STRING
      dropout_rate:
        defaultValue: 0.1
        isOptional: true
        parameterType: NUMBER_DOUBLE
      learning_rate:
        defaultValue: 2.0e-05
        isOptional: true
        parameterType: NUMBER_DOUBLE
      model_name:
        defaultValue: bert-base-uncased
        isOptional: true
        parameterType: STRING
      num_epochs:
        defaultValue: 3.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      project_id:
        parameterType: STRING
      region:
        parameterType: STRING
      weight_decay:
        defaultValue: 0.01
        isOptional: true
        parameterType: NUMBER_DOUBLE
  outputDefinitions:
    artifacts:
      evaluate-model-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      train-model-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0
