{
  "components": {
    "comp-data-prep-stage": {
      "executorLabel": "exec-data-prep-stage",
      "inputDefinitions": {
        "parameters": {
          "code_bucket_path": {
            "parameterType": "STRING"
          },
          "input_path": {
            "parameterType": "STRING"
          },
          "output_dir": {
            "parameterType": "STRING"
          }
        }
      }
    }
  },
  "defaultPipelineRoot": "gs://arsa_model_deployment_uscentral/pipeline_root/",
  "deploymentSpec": {
    "executors": {
      "exec-data-prep-stage": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "data_prep_stage"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn' 'google-cloud-storage' 'torch' 'gcsfs' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef data_prep_stage(\n    code_bucket_path: str,\n    input_path: str,\n    output_dir: str,\n):\n    import os\n    import sys\n    import importlib.util\n    import pandas as pd\n    from google.cloud import storage\n\n    # Logging setup\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    # Download code from GCS\n    client = storage.Client()\n    bucket = client.bucket(code_bucket_path.split('/')[2])\n    prefix = '/'.join(code_bucket_path.split('/')[3:])\n    blobs = client.list_blobs(bucket, prefix=prefix)\n\n    code_dir = \"/tmp/code\"\n    os.makedirs(code_dir, exist_ok=True)\n\n    # for blob in blobs:\n    #     if blob.name.endswith(\".py\"):\n    #         file_path = os.path.join(code_dir, os.path.basename(blob.name))\n    #         blob.download_to_filename(file_path)\n    #         logger.info(f\"Downloaded {blob.name} to {file_path}\")\n\n    for blob in blobs:\n        if blob.name.endswith(\".py\"):\n            # Remove the prefix (e.g., \"code/\") to maintain only the internal folder structure\n            relative_path = blob.name[len(prefix):].lstrip(\"/\")  # Remove the prefix and any leading slashes\n\n            # Create the full path under /tmp/ with the internal folder structure preserved\n            file_path = os.path.join(code_dir, relative_path)\n\n            # Ensure the directory exists\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n            # Download the file\n            blob.download_to_filename(file_path)\n            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n\n    # Log the files in /tmp/code for debugging\n    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n\n    # Add code_dir to sys.path for importing modules\n    sys.path.insert(0, code_dir)\n    logger.info(f\"sys.path updated: {sys.path}\")\n\n    # Import and execute code\n    def load_module_from_file(file_path):\n        module_name = os.path.splitext(os.path.basename(file_path))[0]\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module\n\n    # Load prepare_data.py\n    prepare_data_module = load_module_from_file(f\"{code_dir}/prepare_data.py\")\n\n    # Execute split_and_save_data from prepare_data.py\n    prepare_data_module.split_and_save_data(input_path, output_dir)\n\n    # Upload processed data to GCS\n    output_files = os.listdir(output_dir)\n    for file_name in output_files:\n        local_path = os.path.join(output_dir, file_name)\n        blob_path = f\"output/data/{file_name}\"\n        blob = bucket.blob(blob_path)\n        blob.upload_from_filename(local_path)\n        logger.info(f\"Uploaded {local_path} to gs://{bucket.name}/{blob_path}\")\n\n"
          ],
          "image": "python:3.7"
        }
      }
    }
  },
  "pipelineInfo": {
    "name": "data-prep-stage"
  },
  "root": {
    "dag": {
      "tasks": {
        "data-prep-stage": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-data-prep-stage"
          },
          "inputs": {
            "parameters": {
              "code_bucket_path": {
                "runtimeValue": {
                  "constant": "gs://arsa_model_deployment_uscentral/code/data_prep"
                }
              },
              "input_path": {
                "runtimeValue": {
                  "constant": "gs://arsa_model_deployment_uscentral/input/labeled_data_1perc.csv"
                }
              },
              "output_dir": {
                "runtimeValue": {
                  "constant": "/tmp/output/"
                }
              }
            }
          },
          "taskInfo": {
            "name": "data-prep-stage"
          }
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.4.0"
}