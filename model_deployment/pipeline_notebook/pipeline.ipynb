{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11319/3606101445.py:2: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import dsl\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component, Input, Output, Dataset, Artifact\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "GCP_PROJECT = \"amazonreviewssentimentanalysis\"\n",
    "GCP_REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"arsa_model_deployment_uscentral\"\n",
    "DATA_PATH = f\"gs://{BUCKET_NAME}/input/labeled_data_1perc.csv\"\n",
    "OUTPUT_DIR = f\"gs://{BUCKET_NAME}/output/data/\"\n",
    "CODE_BUCKET_PATH = f\"gs://{BUCKET_NAME}/code\"\n",
    "DATA_PREP_CODE = f\"gs://{BUCKET_NAME}/code/data_prep\"\n",
    "TRAINER_CODE = f\"gs://{BUCKET_NAME}/code/trainer\"\n",
    "MODEL_SAVE_PATH = f\"gs://{BUCKET_NAME}/output/models/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google Cloud Storage client\n",
    "client = storage.Client(project=GCP_PROJECT)\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Function to upload folder to GCS\n",
    "def upload_folder_to_gcs(local_folder, bucket, destination_folder):\n",
    "    # Strip the `gs://<bucket_name>/` prefix from the destination path\n",
    "    if destination_folder.startswith(f\"gs://{bucket.name}/\"):\n",
    "        destination_folder = destination_folder[len(f\"gs://{bucket.name}/\"):]\n",
    "\n",
    "    for root, _, files in os.walk(local_folder):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_folder)\n",
    "            print(local_path,relative_path)\n",
    "\n",
    "            gcs_path = os.path.join(destination_folder, local_path).replace(\"\\\\\", \"/\")\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"Uploaded {local_path} to gs://{bucket.name}/{gcs_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_prep/prepare_data.py prepare_data.py\n",
      "Uploaded data_prep/prepare_data.py to gs://arsa_model_deployment_uscentral/code/data_prep/prepare_data.py\n",
      "data_prep/utils/data_loader.py utils/data_loader.py\n",
      "Uploaded data_prep/utils/data_loader.py to gs://arsa_model_deployment_uscentral/code/data_prep/utils/data_loader.py\n",
      "data_prep/utils/__init__.py utils/__init__.py\n",
      "Uploaded data_prep/utils/__init__.py to gs://arsa_model_deployment_uscentral/code/data_prep/utils/__init__.py\n",
      "trainer/best_hyperparameters.json best_hyperparameters.json\n",
      "Uploaded trainer/best_hyperparameters.json to gs://arsa_model_deployment_uscentral/code/trainer/best_hyperparameters.json\n",
      "trainer/experiment_runner_optuna.py experiment_runner_optuna.py\n",
      "Uploaded trainer/experiment_runner_optuna.py to gs://arsa_model_deployment_uscentral/code/trainer/experiment_runner_optuna.py\n",
      "trainer/train_save.py train_save.py\n",
      "Uploaded trainer/train_save.py to gs://arsa_model_deployment_uscentral/code/trainer/train_save.py\n",
      "trainer/utils/bert_model.py utils/bert_model.py\n",
      "Uploaded trainer/utils/bert_model.py to gs://arsa_model_deployment_uscentral/code/trainer/utils/bert_model.py\n",
      "trainer/utils/data_loader.py utils/data_loader.py\n",
      "Uploaded trainer/utils/data_loader.py to gs://arsa_model_deployment_uscentral/code/trainer/utils/data_loader.py\n",
      "trainer/utils/roberta_model.py utils/roberta_model.py\n",
      "Uploaded trainer/utils/roberta_model.py to gs://arsa_model_deployment_uscentral/code/trainer/utils/roberta_model.py\n",
      "trainer/utils/__init__.py utils/__init__.py\n",
      "Uploaded trainer/utils/__init__.py to gs://arsa_model_deployment_uscentral/code/trainer/utils/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#Upload code to GCP\n",
    "upload_folder_to_gcs(\"data_prep\", bucket, CODE_BUCKET_PATH)\n",
    "upload_folder_to_gcs(\"trainer\", bucket, CODE_BUCKET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"google-cloud-storage\",\"torch\",\"gcsfs\"],\n",
    ")\n",
    "def data_prep_stage(\n",
    "    code_bucket_path: str,\n",
    "    input_path: str,\n",
    "    output_dir: str,\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    import importlib.util\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Logging setup\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Download code from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(code_bucket_path.split('/')[2])\n",
    "    prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "\n",
    "    # for blob in blobs:\n",
    "    #     if blob.name.endswith(\".py\"):\n",
    "    #         file_path = os.path.join(code_dir, os.path.basename(blob.name))\n",
    "    #         blob.download_to_filename(file_path)\n",
    "    #         logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\".py\"):\n",
    "            # Remove the prefix (e.g., \"code/\") to maintain only the internal folder structure\n",
    "            relative_path = blob.name[len(prefix):].lstrip(\"/\")  # Remove the prefix and any leading slashes\n",
    "\n",
    "            # Create the full path under /tmp/ with the internal folder structure preserved\n",
    "            file_path = os.path.join(code_dir, relative_path)\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "            # Download the file\n",
    "            blob.download_to_filename(file_path)\n",
    "            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    # Log the files in /tmp/code for debugging\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "\n",
    "    # Add code_dir to sys.path for importing modules\n",
    "    sys.path.insert(0, code_dir)\n",
    "    logger.info(f\"sys.path updated: {sys.path}\")\n",
    "\n",
    "    # Import and execute code\n",
    "    def load_module_from_file(file_path):\n",
    "        module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    # Load prepare_data.py\n",
    "    prepare_data_module = load_module_from_file(f\"{code_dir}/prepare_data.py\")\n",
    "\n",
    "    # Execute split_and_save_data from prepare_data.py\n",
    "    prepare_data_module.split_and_save_data(input_path, output_dir)\n",
    "\n",
    "    # Upload processed data to GCS\n",
    "    output_files = os.listdir(output_dir)\n",
    "    for file_name in output_files:\n",
    "        local_path = os.path.join(output_dir, file_name)\n",
    "        blob_path = f\"output/data/{file_name}\"\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(local_path)\n",
    "        logger.info(f\"Uploaded {local_path} to gs://{bucket.name}/{blob_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"data-prep-stage\",\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    ")\n",
    "def data_pipeline():\n",
    "    dynamic_code_execution_task = data_prep_stage(\n",
    "        code_bucket_path=DATA_PREP_CODE,\n",
    "        input_path=DATA_PATH,\n",
    "        output_dir=\"/tmp/output/\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data prep and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/661148801406/locations/us-central1/pipelineJobs/data-prep-and-train-20241128122825\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/661148801406/locations/us-central1/pipelineJobs/data-prep-and-train-20241128122825')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/data-prep-and-train-20241128122825?project=661148801406\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"google-cloud-storage\", \"torch\", \"gcsfs\"],\n",
    ")\n",
    "def data_prep_stage(\n",
    "    code_bucket_path: str,\n",
    "    input_path: str,\n",
    "    output_dir: str,\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    import importlib.util\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Logging setup\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Download code from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(code_bucket_path.split('/')[2])\n",
    "    prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    for blob in blobs:\n",
    "        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
    "            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n",
    "            file_path = os.path.join(code_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            blob.download_to_filename(file_path)\n",
    "            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    def load_module_from_file(file_path):\n",
    "        module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    prepare_data_module = load_module_from_file(f\"{code_dir}/prepare_data.py\")\n",
    "    prepare_data_module.split_and_save_data(input_path, output_dir)\n",
    "\n",
    "    # Upload processed data to GCS\n",
    "    output_files = os.listdir(output_dir)\n",
    "    for file_name in output_files:\n",
    "        local_path = os.path.join(output_dir, file_name)\n",
    "        blob_path = f\"output/data/{file_name}\"\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(local_path)\n",
    "        logger.info(f\"Uploaded {local_path} to gs://{bucket.name}/{blob_path}\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"torch\", \"google-cloud-storage\", \"transformers\", \"pandas\", \"scikit-learn\", \"gcsfs\",\"accelerate\"],\n",
    ")\n",
    "def train_save_stage(\n",
    "    code_bucket_path: str,\n",
    "    data_path: str,\n",
    "    model_save_path: str,\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    import importlib.util\n",
    "    from accelerate import Accelerator\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Check available device\n",
    "    logger.info(f\"Using device: {accelerator.device}\")\n",
    "\n",
    "    # Download code from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(code_bucket_path.split('/')[2])\n",
    "    prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    for blob in blobs:\n",
    "        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
    "            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n",
    "            file_path = os.path.join(code_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            blob.download_to_filename(file_path)\n",
    "            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    def load_module_from_file(file_path):\n",
    "        module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    train_save_module = load_module_from_file(f\"{code_dir}/train_save.py\")\n",
    "    hyperparameters_path = os.path.join(code_dir, \"best_hyperparameters.json\")\n",
    "\n",
    "    train_save_module.train_and_save_final_model(\n",
    "        hyperparameters=train_save_module.load_hyperparameters(hyperparameters_path),\n",
    "        data_path=data_path,\n",
    "        model_save_path=model_save_path,\n",
    "    )\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"data-prep-and-train\",\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    ")\n",
    "def data_prep_and_train_pipeline():\n",
    "    # Step 1: Data Preparation\n",
    "    data_prep_task = data_prep_stage(\n",
    "        code_bucket_path=DATA_PREP_CODE,\n",
    "        input_path=DATA_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "    )\n",
    "\n",
    "    # Step 2: Training and Saving Model\n",
    "    train_save_task = train_save_stage(\n",
    "        code_bucket_path=TRAINER_CODE,\n",
    "        data_path=OUTPUT_DIR,\n",
    "        model_save_path=MODEL_SAVE_PATH,\n",
    "    ).set_cpu_limit(\"8\") \\\n",
    "     .set_memory_limit(\"32G\") \\\n",
    "     .set_gpu_limit(1) \\\n",
    "     .set_accelerator_type(\"NVIDIA_TESLA_T4\")\n",
    "\n",
    "    train_save_task.after(data_prep_task)\n",
    "\n",
    "from kfp.v2.compiler import Compiler\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Define the pipeline file path\n",
    "pipeline_file_path = \"data_prep_and_train_pipeline.json\"\n",
    "\n",
    "# Compile the pipeline\n",
    "Compiler().compile(pipeline_func=data_prep_and_train_pipeline, package_path=pipeline_file_path)\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=GCP_PROJECT, location=GCP_REGION)\n",
    "\n",
    "# Submit the pipeline to Vertex AI\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"data-prep-and-train-pipeline\",\n",
    "    template_path=pipeline_file_path,\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    ")\n",
    "\n",
    "pipeline_job.submit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/661148801406/locations/us-central1/pipelineJobs/mlflow-experiment-runner-pipeline-20241124101429\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/661148801406/locations/us-central1/pipelineJobs/mlflow-experiment-runner-pipeline-20241124101429')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlflow-experiment-runner-pipeline-20241124101429?project=661148801406\n"
     ]
    }
   ],
   "source": [
    "# import kfp\n",
    "# from kfp.v2.dsl import component, pipeline\n",
    "# from kfp.v2.compiler import Compiler\n",
    "# from google.cloud import aiplatform\n",
    "\n",
    "# # Environment Variables\n",
    "# GCP_PROJECT = \"amazonreviewssentimentanalysis\"\n",
    "# GCP_REGION = \"us-central1\"\n",
    "# BUCKET_NAME = \"arsa_model_deployment_uscentral\"\n",
    "\n",
    "# # Create MLflow Server Component\n",
    "# @component(\n",
    "#     packages_to_install=[\"google-cloud-compute\", \"mlflow\", \"flask\"],\n",
    "# )\n",
    "# def create_mlflow_server_component(\n",
    "#     vm_name: str,\n",
    "#     region: str,\n",
    "#     zone: str,\n",
    "#     bucket_name: str,\n",
    "#     mlflow_port: int,\n",
    "# ) -> str:\n",
    "#     from google.cloud import compute_v1\n",
    "#     import time\n",
    "#     import os\n",
    "\n",
    "#     # Initialize logging\n",
    "#     import logging\n",
    "#     logging.basicConfig(level=logging.INFO)\n",
    "#     logger = logging.getLogger(__name__)\n",
    "\n",
    "#     # VM Configurations\n",
    "#     project = os.getenv(\"GCP_PROJECT\")\n",
    "#     instance_client = compute_v1.InstancesClient()\n",
    "#     machine_type = f\"zones/{zone}/machineTypes/e2-micro\"\n",
    "#     disk_image = \"projects/debian-cloud/global/images/family/debian-10\"\n",
    "#     startup_script = f\"\"\"#!/bin/bash\n",
    "#     apt-get update\n",
    "#     apt-get install -y python3-pip\n",
    "#     pip3 install mlflow flask google-cloud-storage\n",
    "#     nohup mlflow server \\\n",
    "#         --backend-store-uri sqlite:///mlflow.db \\\n",
    "#         --default-artifact-root gs://{bucket_name}/mlflow-artifacts/ \\\n",
    "#         --host 0.0.0.0 \\\n",
    "#         --port {mlflow_port} &\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Create the VM\n",
    "#     instance = compute_v1.Instance()\n",
    "#     instance.name = vm_name\n",
    "#     instance.zone = zone\n",
    "#     instance.machine_type = machine_type\n",
    "#     instance.network_interfaces = [{\"name\": \"global/networks/default\"}]\n",
    "#     instance.disks = [\n",
    "#         {\n",
    "#             \"boot\": True,\n",
    "#             \"auto_delete\": True,\n",
    "#             \"initialize_params\": {\n",
    "#                 \"source_image\": disk_image,\n",
    "#                 \"disk_size_gb\": 10,\n",
    "#             },\n",
    "#         }\n",
    "#     ]\n",
    "#     instance.metadata = {\"items\": [{\"key\": \"startup-script\", \"value\": startup_script}]}\n",
    "\n",
    "#     # Insert the instance\n",
    "#     operation = instance_client.insert_unary(\n",
    "#         project=project, zone=zone, instance_resource=instance\n",
    "#     )\n",
    "#     logger.info(f\"Creating VM {vm_name}, operation: {operation}\")\n",
    "#     time.sleep(60)  # Wait for the VM to start\n",
    "\n",
    "#     # Get the external IP of the VM\n",
    "#     vm = instance_client.get(project=project, zone=zone, instance=vm_name)\n",
    "#     external_ip = vm.network_interfaces[0].access_configs[0].nat_ip\n",
    "\n",
    "#     # Return the MLflow URI\n",
    "#     mlflow_uri = f\"http://{external_ip}:{mlflow_port}\"\n",
    "#     logger.info(f\"MLflow server is available at {mlflow_uri}\")\n",
    "#     return mlflow_uri\n",
    "\n",
    "\n",
    "# # Experiment Runner Component\n",
    "# @component(\n",
    "#     packages_to_install=[\n",
    "#         \"optuna\",\n",
    "#         \"mlflow\",\n",
    "#         \"torch\",\n",
    "#         \"transformers\",\n",
    "#         \"scikit-learn\",\n",
    "#         \"pandas\",\n",
    "#         \"google-cloud-storage\",\n",
    "#         \"gcsfs\",\n",
    "#     ],\n",
    "# )\n",
    "# def experiment_runner_component(\n",
    "#     code_bucket_path: str,\n",
    "#     mlflow_tracking_uri: str,\n",
    "#     dataset_path: str,\n",
    "#     output_hyperparams_path: str,\n",
    "# ):\n",
    "#     import os\n",
    "#     import sys\n",
    "#     import subprocess\n",
    "#     from google.cloud import storage\n",
    "\n",
    "#     # Logging setup\n",
    "#     import logging\n",
    "#     logging.basicConfig(level=logging.INFO)\n",
    "#     logger = logging.getLogger(__name__)\n",
    "\n",
    "#     # Download code from GCS\n",
    "#     client = storage.Client()\n",
    "#     bucket_name = code_bucket_path.split(\"/\")[2]\n",
    "#     prefix = \"/\".join(code_bucket_path.split(\"/\")[3:])\n",
    "#     bucket = client.bucket(bucket_name)\n",
    "#     blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "#     code_dir = \"/tmp/code\"\n",
    "#     os.makedirs(code_dir, exist_ok=True)\n",
    "\n",
    "#     for blob in blobs:\n",
    "#         if blob.name.endswith(\".py\"):\n",
    "#             # Maintain folder structure\n",
    "#             relative_path = blob.name[len(prefix) :].lstrip(\"/\")\n",
    "#             file_path = os.path.join(code_dir, relative_path)\n",
    "#             os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#             blob.download_to_filename(file_path)\n",
    "#             logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "#     # Add code_dir to sys.path\n",
    "#     sys.path.insert(0, code_dir)\n",
    "#     logger.info(f\"sys.path updated: {sys.path}\")\n",
    "\n",
    "#     # Set MLflow tracking URI\n",
    "#     os.environ[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n",
    "#     logger.info(f\"MLflow tracking URI set to: {mlflow_tracking_uri}\")\n",
    "\n",
    "#     # Run the experiment\n",
    "#     try:\n",
    "#         script_path = os.path.join(code_dir, \"experiment_runner_optuna.py\")\n",
    "#         subprocess.run([\"python3\", script_path, \"--data_path\", dataset_path], check=True)\n",
    "\n",
    "#         # Move the best hyperparameters file to the output path\n",
    "#         hyperparams_local_path = os.path.join(code_dir, \"best_hyperparameters.json\")\n",
    "#         storage_path = os.path.join(output_hyperparams_path, \"best_hyperparameters.json\")\n",
    "#         if os.path.exists(hyperparams_local_path):\n",
    "#             output_bucket = client.bucket(bucket_name)\n",
    "#             blob = output_bucket.blob(storage_path)\n",
    "#             blob.upload_from_filename(hyperparams_local_path)\n",
    "#             logger.info(f\"Uploaded best hyperparameters to: gs://{bucket_name}/{storage_path}\")\n",
    "#         else:\n",
    "#             logger.warning(\"best_hyperparameters.json not found.\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Experiment failed: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # Define the Pipeline\n",
    "# @pipeline(\n",
    "#     name=\"mlflow-experiment-runner-pipeline\",\n",
    "#     pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    "# )\n",
    "# def mlflow_pipeline(\n",
    "#     vm_name: str = \"mlflow-server\",\n",
    "#     region: str = GCP_REGION,\n",
    "#     zone: str = \"us-central1-a\",\n",
    "#     bucket_name: str = BUCKET_NAME,\n",
    "#     mlflow_port: int = 5000,\n",
    "#     dataset_path: str = DATA_PATH,\n",
    "# ):\n",
    "#     # Step 1: Create MLflow Server\n",
    "#     mlflow_server_task = create_mlflow_server_component(\n",
    "#         vm_name=vm_name,\n",
    "#         region=region,\n",
    "#         zone=zone,\n",
    "#         bucket_name=bucket_name,\n",
    "#         mlflow_port=mlflow_port,\n",
    "#     )\n",
    "\n",
    "#     # Step 2: Run Experiment\n",
    "#     experiment_runner_task = experiment_runner_component(\n",
    "#         code_bucket_path=f\"gs://{BUCKET_NAME}/code/\",\n",
    "#         mlflow_tracking_uri=mlflow_server_task.output,\n",
    "#         dataset_path=dataset_path,\n",
    "#         output_hyperparams_path=f\"gs://{BUCKET_NAME}/output/hyperparams/\",\n",
    "#     ).set_cpu_limit(\"4\").set_memory_limit(\"16Gi\")\n",
    "\n",
    "# # Compile the pipeline\n",
    "# pipeline_file_path = \"mlflow_pipeline.json\"\n",
    "# Compiler().compile(pipeline_func=mlflow_pipeline, package_path=pipeline_file_path)\n",
    "\n",
    "# # Submit the pipeline\n",
    "# aiplatform.init(\n",
    "#     project=GCP_PROJECT,\n",
    "#     location=GCP_REGION,\n",
    "#     staging_bucket=f\"gs://{BUCKET_NAME}\",\n",
    "# )\n",
    "\n",
    "# pipeline_job = aiplatform.PipelineJob(\n",
    "#     display_name=\"mlflow-experiment-pipeline\",\n",
    "#     template_path=pipeline_file_path,\n",
    "#     pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    "# )\n",
    "\n",
    "# pipeline_job.submit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/661148801406/locations/us-central1/pipelineJobs/experiment-runner-pipeline-20241124094851\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/661148801406/locations/us-central1/pipelineJobs/experiment-runner-pipeline-20241124094851')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/experiment-runner-pipeline-20241124094851?project=661148801406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Vertex AI client\n",
    "aiplatform.init(\n",
    "    project=GCP_PROJECT,\n",
    "    location=GCP_REGION,\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}\",\n",
    ")\n",
    "\n",
    "# Submit the pipeline job\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"experiment-runner-pipeline\",\n",
    "    template_path=pipeline_file_path,\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    "    parameter_values={\n",
    "        \"machine_type\": \"e2-standard-4\",  # Specify machine type dynamically\n",
    "    },\n",
    ")\n",
    "\n",
    "pipeline_job.submit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'compute_v1' from 'google.cloud' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_v1\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_active_vms\u001b[39m(project_id, region):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Logs active VMs in the given project and region.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        region (str): GCP region (e.g., \"us-central1\").\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'compute_v1' from 'google.cloud' (unknown location)"
     ]
    }
   ],
   "source": [
    "from google.cloud import compute_v1\n",
    "\n",
    "def log_active_vms(project_id, region):\n",
    "    \"\"\"\n",
    "    Logs active VMs in the given project and region.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID.\n",
    "        region (str): GCP region (e.g., \"us-central1\").\n",
    "    \"\"\"\n",
    "    client = compute_v1.InstancesClient()\n",
    "    zones_client = compute_v1.ZonesClient()\n",
    "\n",
    "    # Get all zones in the region\n",
    "    zones = [\n",
    "        zone.name\n",
    "        for zone in zones_client.list(project=project_id)\n",
    "        if zone.name.startswith(region)\n",
    "    ]\n",
    "\n",
    "    print(f\"Checking active VMs in project '{project_id}' and region '{region}'...\")\n",
    "    for zone in zones:\n",
    "        instances = client.list(project=project_id, zone=zone)\n",
    "        for instance in instances:\n",
    "            print(\n",
    "                f\"Instance: {instance.name}, Zone: {zone}, Status: {instance.status}, Machine Type: {instance.machine_type}\"\n",
    "            )\n",
    "\n",
    "# Call the function to log active VMs\n",
    "log_active_vms(GCP_PROJECT, GCP_REGION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
