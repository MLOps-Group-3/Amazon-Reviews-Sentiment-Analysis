{
  "components": {
    "comp-data-prep-stage": {
      "executorLabel": "exec-data-prep-stage",
      "inputDefinitions": {
        "parameters": {
          "code_bucket_path": {
            "parameterType": "STRING"
          },
          "input_path": {
            "parameterType": "STRING"
          },
          "output_dir": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-train-save-stage": {
      "executorLabel": "exec-train-save-stage",
      "inputDefinitions": {
        "parameters": {
          "code_bucket_path": {
            "parameterType": "STRING"
          },
          "data_path": {
            "parameterType": "STRING"
          },
          "model_save_path": {
            "parameterType": "STRING"
          }
        }
      }
    }
  },
  "defaultPipelineRoot": "gs://arsa_model_deployment_uscentral/pipeline_root/",
  "deploymentSpec": {
    "executors": {
      "exec-data-prep-stage": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "data_prep_stage"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn' 'google-cloud-storage' 'torch' 'gcsfs' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef data_prep_stage(\n    code_bucket_path: str,\n    input_path: str,\n    output_dir: str,\n):\n    import os\n    import sys\n    import importlib.util\n    import pandas as pd\n    from google.cloud import storage\n\n    # Logging setup\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    # Download code from GCS\n    client = storage.Client()\n    bucket = client.bucket(code_bucket_path.split('/')[2])\n    prefix = '/'.join(code_bucket_path.split('/')[3:])\n    blobs = client.list_blobs(bucket, prefix=prefix)\n\n    code_dir = \"/tmp/code\"\n    os.makedirs(code_dir, exist_ok=True)\n    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n\n    for blob in blobs:\n        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n            file_path = os.path.join(code_dir, relative_path)\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n            blob.download_to_filename(file_path)\n            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n\n    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n    sys.path.insert(0, code_dir)\n\n    def load_module_from_file(file_path):\n        module_name = os.path.splitext(os.path.basename(file_path))[0]\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module\n\n    prepare_data_module = load_module_from_file(f\"{code_dir}/prepare_data.py\")\n    prepare_data_module.split_and_save_data(input_path, output_dir)\n\n    # Upload processed data to GCS\n    output_files = os.listdir(output_dir)\n    for file_name in output_files:\n        local_path = os.path.join(output_dir, file_name)\n        blob_path = f\"output/data/{file_name}\"\n        blob = bucket.blob(blob_path)\n        blob.upload_from_filename(local_path)\n        logger.info(f\"Uploaded {local_path} to gs://{bucket.name}/{blob_path}\")\n\n"
          ],
          "image": "python:3.7"
        }
      },
      "exec-train-save-stage": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "train_save_stage"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'torch' 'google-cloud-storage' 'transformers' 'pandas' 'scikit-learn' 'gcsfs' 'accelerate' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef train_save_stage(\n    code_bucket_path: str,\n    data_path: str,\n    model_save_path: str,\n):\n    import os\n    import sys\n    import logging\n    from google.cloud import storage\n    import importlib.util\n    from accelerate import Accelerator\n\n\n    # Logging setup\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    # Initialize Accelerator\n    accelerator = Accelerator()\n\n    # Check available device\n    logger.info(f\"Using device: {accelerator.device}\")\n\n    # Download code from GCS\n    client = storage.Client()\n    bucket = client.bucket(code_bucket_path.split('/')[2])\n    prefix = '/'.join(code_bucket_path.split('/')[3:])\n    blobs = client.list_blobs(bucket, prefix=prefix)\n\n    code_dir = \"/tmp/code\"\n    os.makedirs(code_dir, exist_ok=True)\n    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n\n    for blob in blobs:\n        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n            file_path = os.path.join(code_dir, relative_path)\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n            blob.download_to_filename(file_path)\n            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n\n    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n    sys.path.insert(0, code_dir)\n\n    def load_module_from_file(file_path):\n        module_name = os.path.splitext(os.path.basename(file_path))[0]\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module\n\n    train_save_module = load_module_from_file(f\"{code_dir}/train_save.py\")\n    hyperparameters_path = os.path.join(code_dir, \"best_hyperparameters.json\")\n\n    train_save_module.train_and_save_final_model(\n        hyperparameters=train_save_module.load_hyperparameters(hyperparameters_path),\n        data_path=data_path,\n        model_save_path=model_save_path,\n    )\n\n"
          ],
          "image": "python:3.7",
          "resources": {
            "accelerator": {
              "count": "1",
              "type": "NVIDIA_TESLA_T4"
            },
            "cpuLimit": 8.0,
            "memoryLimit": 32.0
          }
        }
      }
    }
  },
  "pipelineInfo": {
    "name": "data-prep-and-train"
  },
  "root": {
    "dag": {
      "tasks": {
        "data-prep-stage": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-data-prep-stage"
          },
          "inputs": {
            "parameters": {
              "code_bucket_path": {
                "runtimeValue": {
                  "constant": "gs://arsa_model_deployment_uscentral/code/data_prep"
                }
              },
              "input_path": {
                "runtimeValue": {
                  "constant": "gs://arsa_model_deployment_uscentral/input/labeled_data_1perc.csv"
                }
              },
              "output_dir": {
                "runtimeValue": {
                  "constant": "gs://arsa_model_deployment_uscentral/output/data/"
                }
              }
            }
          },
          "taskInfo": {
            "name": "data-prep-stage"
          }
        },
        "train-save-stage": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-train-save-stage"
          },
          "dependentTasks": [
            "data-prep-stage"
          ],
          "inputs": {
            "parameters": {
              "code_bucket_path": {
                "runtimeValue": {
                  "constant": "gs://arsa_model_deployment_uscentral/code/trainer"
                }
              },
              "data_path": {
                "runtimeValue": {
                  "constant": "gs://arsa_model_deployment_uscentral/output/data/"
                }
              },
              "model_save_path": {
                "runtimeValue": {
                  "constant": "gs://arsa_model_deployment_uscentral/output/models/"
                }
              }
            }
          },
          "taskInfo": {
            "name": "train-save-stage"
          }
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.4.0"
}