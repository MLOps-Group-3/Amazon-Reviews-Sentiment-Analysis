{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19653/3843122684.py:2: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import dsl\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component, Input, Output, Dataset, Artifact\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from google.cloud import aiplatform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "GCP_PROJECT = \"amazonreviewssentimentanalysis\"\n",
    "GCP_REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"arsa_model_deployment_uscentral_v2\"\n",
    "DATA_PATH = f\"gs://{BUCKET_NAME}/input/labeled_data_10perc.csv\"\n",
    "OUTPUT_DIR = f\"gs://{BUCKET_NAME}/output/data/\"\n",
    "CODE_BUCKET_PATH = f\"gs://{BUCKET_NAME}/code\"\n",
    "# DATA_PREP_CODE = f\"gs://{BUCKET_NAME}/code/data_prep\"\n",
    "SOURCE_CODE = f\"gs://{BUCKET_NAME}/code/src\"\n",
    "SLICE_METRIC_PATH = f\"gs://{BUCKET_NAME}/output/metrics\"\n",
    "# TRAINER_CODE = f\"gs://{BUCKET_NAME}/code/trainer\"\n",
    "MODEL_SAVE_PATH = f\"gs://{BUCKET_NAME}/output/models/final_model.pth\"\n",
    "# TORCH_SERVE_PATH = f\"gs://{BUCKET_NAME}/code/predictor/\"\n",
    "VERSION = 1\n",
    "APP_NAME = \"review_sentiment_bert_model\"\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"{APP_NAME}-v{VERSION}\"\n",
    "MODEL_DESCRIPTION = \"PyTorch serve deployment model for amazon reviews sentiment classification\"\n",
    "\n",
    "# MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{APP_NAME}\"\n",
    "serving_container_ports = [7080]\n",
    "\n",
    "PROJECT_ID = \"amazonreviewssentimentanalysis\" \n",
    "APP_NAME = \"review_sentiment_bert_model\"\n",
    "DOCKER_IMAGE_NAME = \"pytorch_predict_{APP_NAME}\"\n",
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google Cloud Storage client\n",
    "client = storage.Client(project=GCP_PROJECT)\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Function to upload folder to GCS\n",
    "def upload_folder_to_gcs(local_folder, bucket, destination_folder):\n",
    "    # Strip the `gs://<bucket_name>/` prefix from the destination path\n",
    "    if destination_folder.startswith(f\"gs://{bucket.name}/\"):\n",
    "        destination_folder = destination_folder[len(f\"gs://{bucket.name}/\"):]\n",
    "\n",
    "    for root, _, files in os.walk(local_folder):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_folder)\n",
    "            print(local_path,relative_path)\n",
    "\n",
    "            gcs_path = os.path.join(destination_folder, local_path).replace(\"\\\\\", \"/\")\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"Uploaded {local_path} to gs://{bucket.name}/{gcs_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/app.py app.py\n",
      "Uploaded src/app.py to gs://arsa_model_deployment_uscentral_v2/code/src/app.py\n",
      "src/app_requirements.txt app_requirements.txt\n",
      "Uploaded src/app_requirements.txt to gs://arsa_model_deployment_uscentral_v2/code/src/app_requirements.txt\n",
      "src/bert_model_torch_serve.py bert_model_torch_serve.py\n",
      "Uploaded src/bert_model_torch_serve.py to gs://arsa_model_deployment_uscentral_v2/code/src/bert_model_torch_serve.py\n",
      "src/best_hyperparameters.json best_hyperparameters.json\n",
      "Uploaded src/best_hyperparameters.json to gs://arsa_model_deployment_uscentral_v2/code/src/best_hyperparameters.json\n",
      "src/bias_detect.py bias_detect.py\n",
      "Uploaded src/bias_detect.py to gs://arsa_model_deployment_uscentral_v2/code/src/bias_detect.py\n",
      "src/config.py config.py\n",
      "Uploaded src/config.py to gs://arsa_model_deployment_uscentral_v2/code/src/config.py\n",
      "src/custom_handler.py custom_handler.py\n",
      "Uploaded src/custom_handler.py to gs://arsa_model_deployment_uscentral_v2/code/src/custom_handler.py\n",
      "src/Dockerfile Dockerfile\n",
      "Uploaded src/Dockerfile to gs://arsa_model_deployment_uscentral_v2/code/src/Dockerfile\n",
      "src/evaluate_model.py evaluate_model.py\n",
      "Uploaded src/evaluate_model.py to gs://arsa_model_deployment_uscentral_v2/code/src/evaluate_model.py\n",
      "src/evaluate_model_slices.py evaluate_model_slices.py\n",
      "Uploaded src/evaluate_model_slices.py to gs://arsa_model_deployment_uscentral_v2/code/src/evaluate_model_slices.py\n",
      "src/experiment_runner.py experiment_runner.py\n",
      "Uploaded src/experiment_runner.py to gs://arsa_model_deployment_uscentral_v2/code/src/experiment_runner.py\n",
      "src/experiment_runner_optuna.py experiment_runner_optuna.py\n",
      "Uploaded src/experiment_runner_optuna.py to gs://arsa_model_deployment_uscentral_v2/code/src/experiment_runner_optuna.py\n",
      "src/index_to_name.json index_to_name.json\n",
      "Uploaded src/index_to_name.json to gs://arsa_model_deployment_uscentral_v2/code/src/index_to_name.json\n",
      "src/prepare_data.py prepare_data.py\n",
      "Uploaded src/prepare_data.py to gs://arsa_model_deployment_uscentral_v2/code/src/prepare_data.py\n",
      "src/README.md README.md\n",
      "Uploaded src/README.md to gs://arsa_model_deployment_uscentral_v2/code/src/README.md\n",
      "src/train_save.py train_save.py\n",
      "Uploaded src/train_save.py to gs://arsa_model_deployment_uscentral_v2/code/src/train_save.py\n",
      "src/data/label_encoder.pkl data/label_encoder.pkl\n",
      "Uploaded src/data/label_encoder.pkl to gs://arsa_model_deployment_uscentral_v2/code/src/data/label_encoder.pkl\n",
      "src/data/slice_metrics.csv data/slice_metrics.csv\n",
      "Uploaded src/data/slice_metrics.csv to gs://arsa_model_deployment_uscentral_v2/code/src/data/slice_metrics.csv\n",
      "src/data/test.pkl data/test.pkl\n",
      "Uploaded src/data/test.pkl to gs://arsa_model_deployment_uscentral_v2/code/src/data/test.pkl\n",
      "src/data/train.pkl data/train.pkl\n",
      "Uploaded src/data/train.pkl to gs://arsa_model_deployment_uscentral_v2/code/src/data/train.pkl\n",
      "src/data/val.pkl data/val.pkl\n",
      "Uploaded src/data/val.pkl to gs://arsa_model_deployment_uscentral_v2/code/src/data/val.pkl\n",
      "src/utils/bert_model.py utils/bert_model.py\n",
      "Uploaded src/utils/bert_model.py to gs://arsa_model_deployment_uscentral_v2/code/src/utils/bert_model.py\n",
      "src/utils/data_loader.py utils/data_loader.py\n",
      "Uploaded src/utils/data_loader.py to gs://arsa_model_deployment_uscentral_v2/code/src/utils/data_loader.py\n",
      "src/utils/roberta_model.py utils/roberta_model.py\n",
      "Uploaded src/utils/roberta_model.py to gs://arsa_model_deployment_uscentral_v2/code/src/utils/roberta_model.py\n",
      "src/utils/__init__.py utils/__init__.py\n",
      "Uploaded src/utils/__init__.py to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__init__.py\n",
      "src/utils/__pycache__/bert_model.cpython-312.pyc utils/__pycache__/bert_model.cpython-312.pyc\n",
      "Uploaded src/utils/__pycache__/bert_model.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__pycache__/bert_model.cpython-312.pyc\n",
      "src/utils/__pycache__/data_loader.cpython-312.pyc utils/__pycache__/data_loader.cpython-312.pyc\n",
      "Uploaded src/utils/__pycache__/data_loader.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__pycache__/data_loader.cpython-312.pyc\n",
      "src/utils/__pycache__/roberta_model.cpython-312.pyc utils/__pycache__/roberta_model.cpython-312.pyc\n",
      "Uploaded src/utils/__pycache__/roberta_model.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__pycache__/roberta_model.cpython-312.pyc\n",
      "src/utils/__pycache__/__init__.cpython-312.pyc utils/__pycache__/__init__.cpython-312.pyc\n",
      "Uploaded src/utils/__pycache__/__init__.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__pycache__/__init__.cpython-312.pyc\n",
      "src/__pycache__/config.cpython-312.pyc __pycache__/config.cpython-312.pyc\n",
      "Uploaded src/__pycache__/config.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/__pycache__/config.cpython-312.pyc\n",
      "src/__pycache__/evaluate_model.cpython-312.pyc __pycache__/evaluate_model.cpython-312.pyc\n",
      "Uploaded src/__pycache__/evaluate_model.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/__pycache__/evaluate_model.cpython-312.pyc\n"
     ]
    }
   ],
   "source": [
    "upload_folder_to_gcs(\"src\", bucket, CODE_BUCKET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/661148801406/locations/us-central1/pipelineJobs/data-prep-and-train-20241202184756\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/661148801406/locations/us-central1/pipelineJobs/data-prep-and-train-20241202184756')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/data-prep-and-train-20241202184756?project=661148801406\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    Metrics,\n",
    "    component,\n",
    "    pipeline,\n",
    ")\n",
    "from typing import NamedTuple\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"google-cloud-storage\", \"torch\", \"gcsfs\",\"arsa-pipeline-tools\"],\n",
    ")\n",
    "def data_prep_stage(\n",
    "    code_bucket_path: str,\n",
    "    input_path: str,\n",
    "    output_dir: str,\n",
    "    train_data: Output[Dataset],\n",
    "    val_data: Output[Dataset],\n",
    "    test_data: Output[Dataset],\n",
    "\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    import importlib.util\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from arsa_pipeline_tools.utils import download_files_from_gcs, load_module_from_file\n",
    "    # Logging setup\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "\n",
    "    download_files_from_gcs(code_bucket_path,code_dir)\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    prepare_data_module = load_module_from_file(f\"{code_dir}/prepare_data.py\")\n",
    "    train_df, val_df, test_df, label_encoder = prepare_data_module.split_and_save_data(input_path, output_dir)\n",
    "    train_df.to_pickle(train_data.path)\n",
    "    val_df.to_pickle(val_data.path)\n",
    "    test_df.to_pickle(test_data.path)\n",
    "    # label_encoder.to_pickle(label_encoder_data.path)\n",
    "    logger.info(\"Artifacts for train, dev, and test data created successfully.\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    # packages_to_install=[\n",
    "    #     \"optuna\",\n",
    "    #     \"mlflow\",\n",
    "    #     \"torch==1.12.1\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "    #     \"transformers==4.21.0\",  # Compatible with PyTorch 1.12\n",
    "    #     \"numpy\",\n",
    "    #     \"google-cloud-storage\",\n",
    "    #     \"scikit-learn\",\n",
    "    #     \"arsa-pipeline-tools\",\n",
    "    # ],\n",
    "    # base_image=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-12.py310:latest\",  # Python 3.10 with GPU support\n",
    "    packages_to_install=[\n",
    "        \"optuna\",\n",
    "        \"mlflow\",\n",
    "        \"torch\",\n",
    "        \"transformers[torch]\",  # Compatible with PyTorch 1.12\n",
    "        \"numpy\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"scikit-learn\",\n",
    "        \"arsa-pipeline-tools\",\n",
    "    ],\n",
    "    base_image=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu118.py310\",  # Python 3.10 with GPU support\n",
    "\n",
    ")\n",
    "def run_optuna_experiment(\n",
    "    code_bucket_path: str,\n",
    "    data_path: str,\n",
    "    train_data: Input[Dataset],\n",
    "    val_data: Input[Dataset],\n",
    "    test_data: Input[Dataset],\n",
    "    best_hyperparams_metrics: Output[Metrics],\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    import importlib.util\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    from arsa_pipeline_tools.utils import download_files_from_gcs, load_module_from_file\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\"}\n",
    "\n",
    "    download_files_from_gcs(code_bucket_path,code_dir,ALLOWED_EXTENSIONS)\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    # Ensure `experiment_runner_optuna.py` exists\n",
    "    script_path = os.path.join(code_dir, \"experiment_runner_optuna.py\")\n",
    "    if not os.path.exists(script_path):\n",
    "        raise FileNotFoundError(f\"`experiment_runner_optuna.py` not found in {code_dir}\")\n",
    "\n",
    "    # Load and execute the experiment\n",
    "    experiment_module = load_module_from_file(script_path)\n",
    "\n",
    "    # Run the Optuna experiment\n",
    "    best_hyperparameters = experiment_module.find_best_hyperparameters(data_path)\n",
    "    logger.info(best_hyperparameters)\n",
    "    # Save the best hyperparameters to the output artifact\n",
    "    # Log hyperparameters to Metrics artifact\n",
    "    for key, value in best_hyperparameters.items():\n",
    "        best_hyperparams_metrics.log_metric(key, value)\n",
    "\n",
    "@component(\n",
    "    # packages_to_install=[\"torch\", \"google-cloud-storage\", \"transformers\", \"pandas\", \"scikit-learn\", \"gcsfs\",\"accelerate\"],\n",
    "    # packages_to_install=[\n",
    "    #     \"pandas\",\n",
    "    #     \"torch==1.12.1\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "    #     \"transformers==4.21.0\",  # Compatible with PyTorch 1.12\n",
    "    #     \"scikit-learn\",\n",
    "    #     \"accelerate==0.12.0\",  # Compatible with PyTorch 1.12 and transformers\n",
    "    #     \"google-cloud-storage\",\n",
    "    #     # \"kfp==2.0.0\",  # Compatible version of kfp\n",
    "    #     \"PyYAML>=6.0\",  # A stable version compatible with the other libraries\n",
    "    #     \"tensorboard\",\n",
    "    #     \"arsa-pipeline-tools\",\n",
    "    # ],\n",
    "    # base_image=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-12.py310:latest\",  # Python 3.10 with GPU support\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"torch\",\n",
    "        \"transformers[torch]\",  # Compatible with PyTorch 1.12\n",
    "        \"scikit-learn\",\n",
    "        # \"accelerate==0.12.0\",  # Compatible with PyTorch 1.12 and transformers\n",
    "        \"google-cloud-storage\",\n",
    "        # \"kfp==2.0.0\",  # Compatible version of kfp\n",
    "        \"PyYAML>=6.0\",  # A stable version compatible with the other libraries\n",
    "        \"tensorboard\",\n",
    "        \"arsa-pipeline-tools\",\n",
    "    ],\n",
    "    base_image=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu118.py310\",  # Python 3.10 with GPU support\n",
    "\n",
    ")\n",
    "def train_save_stage(\n",
    "    code_bucket_path: str,\n",
    "    data_path: str,\n",
    "    model_save_path: str,\n",
    "    train_data: Input[Dataset],\n",
    "    val_data: Input[Dataset],\n",
    "    best_hyperparams_metrics: Input[Metrics],\n",
    "    model: Output[Model],\n",
    "    model_metrics: Output[Metrics],\n",
    "    \n",
    "\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    import importlib.util\n",
    "    from accelerate import Accelerator\n",
    "    from arsa_pipeline_tools.utils import download_files_from_gcs, load_module_from_file\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Check available device\n",
    "    logger.info(f\"Using device: {accelerator.device}\")\n",
    "\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    download_files_from_gcs(code_bucket_path,code_dir,ALLOWED_EXTENSIONS)\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    train_save_module = load_module_from_file(f\"{code_dir}/train_save.py\")\n",
    "    hyperparameters_path = os.path.join(code_dir, \"best_hyperparameters.json\")\n",
    "\n",
    "    best_hyperparams = {key: value for key, value in best_hyperparams_metrics.metadata.items()}\n",
    "    logger.info(f\"Read best hyperparameters from metrics: {best_hyperparams}\")\n",
    "\n",
    "    returned_model_path, epoch_metrics = train_save_module.train_and_save_final_model(\n",
    "        hyperparameters=best_hyperparams,  #train_save_module.load_hyperparameters(hyperparameters_path),\n",
    "        data_path=data_path,\n",
    "        train_data = train_data,\n",
    "        val_data = val_data, \n",
    "        model_save_path=model_save_path,\n",
    "    )\n",
    "\n",
    "\n",
    "    model.metadata[\"gcs_path\"] = returned_model_path\n",
    "    logger.info(f\"Model artifact metadata updated with GCS path: {returned_model_path}\")\n",
    "\n",
    "    print(epoch_metrics)\n",
    "    logger.info(f\"epoch_metrics: {epoch_metrics}\")\n",
    "    # Log metrics to the Vertex AI UI\n",
    "    # Corrected logging for Vertex AI\n",
    "    for epoch, metric in enumerate(epoch_metrics, start=1):\n",
    "        # Log accuracy and loss (ensure keys match)\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_accuracy\", metric[\"eval_accuracy\"])\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_loss\", metric[\"eval_loss\"])\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_precision\", metric[\"eval_precision\"])\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_recall\", metric[\"eval_recall\"])\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_f1\", metric[\"eval_f1\"])\n",
    "\n",
    "        # metrics.log_metric(f\"epoch_{epoch}_runtime\", metric[\"eval_runtime\"])\n",
    "        # metrics.log_metric(f\"epoch_{epoch}_samples_per_second\", metric[\"eval_samples_per_second\"])\n",
    "        # metrics.log_metric(f\"epoch_{epoch}_steps_per_second\", metric[\"eval_steps_per_second\"])\n",
    "        \n",
    "        # Log to standard output\n",
    "        logger.info(f\"Logged metrics for epoch {epoch}: {metric}\")\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"torch\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "        \"transformers[torch]\",  # Compatible with PyTorch 1.12\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"gcsfs\",\n",
    "        \"arsa-pipeline-tools\",\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def evaluate_model_component(\n",
    "    code_bucket_path: str,\n",
    "    model_gcs_path: Input[Model],\n",
    "    test_data: Input[Dataset],\n",
    "    eval_metrics: Output[Metrics],\n",
    "    # f1_score: Output[float],\n",
    "    f1_threshold: float = 0.6,\n",
    ")-> NamedTuple(\"output\", [(\"eval_pass\", str)]):\n",
    "    import logging\n",
    "    import json\n",
    "    import importlib.util\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import sys\n",
    "    from arsa_pipeline_tools.utils import download_files_from_gcs, load_module_from_file\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "    download_files_from_gcs(code_bucket_path,code_dir,ALLOWED_EXTENSIONS)\n",
    "\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    # Ensure `evaluate_model.py` exists\n",
    "    evaluate_script_path = os.path.join(code_dir, \"evaluate_model.py\")\n",
    "    if not os.path.exists(evaluate_script_path):\n",
    "        raise FileNotFoundError(f\"`evaluate_model.py` not found in {code_dir}\")\n",
    "\n",
    "    # Load `evaluate_model.py` dynamically\n",
    "    evaluate_module = load_module_from_file(evaluate_script_path)\n",
    "\n",
    "    logger.info(f\"model_gcs_path : {model_gcs_path},\\t model_gcs_path.uri {model_gcs_path.uri}, metadata {model_gcs_path.metadata['gcs_path']}\")\n",
    "    # Call `gcp_eval` method from the module\n",
    "    accuracy, precision, recall, f1 = evaluate_module.gcp_eval(\n",
    "        test_df=test_data,\n",
    "        model_path=model_gcs_path.metadata[\"gcs_path\"],\n",
    "    )\n",
    "\n",
    "    # Log metrics to Vertex AI\n",
    "    eval_metrics.log_metric(\"accuracy\", accuracy)\n",
    "    eval_metrics.log_metric(\"precision\", precision)\n",
    "    eval_metrics.log_metric(\"recall\", recall)\n",
    "    eval_metrics.log_metric(\"f1\", f1)\n",
    "    # Conditional check\n",
    "    if f1 >= f1_threshold:\n",
    "        logger.info(f\"Model passed the F1 threshold: {f1:.4f} >= {f1_threshold}\")\n",
    "        eval_pass = \"true\"\n",
    "        return (eval_pass,)\n",
    "    else:\n",
    "        logger.error(f\"Model failed to meet the F1 threshold: {f1:.4f} < {f1_threshold}\")\n",
    "        eval_pass = \"false\"\n",
    "        return (eval_pass,)\n",
    "\n",
    "        # raise ValueError(f\"F1 score {f1:.4f} is below the threshold {f1_threshold}\")\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"torch\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "        \"transformers[torch]\",  # Compatible with PyTorch 1.12\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"gcsfs\",\n",
    "        \"arsa-pipeline-tools\",\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def evaluate_slices_component(\n",
    "    code_bucket_path: str,\n",
    "    model_gcs_path: Input[Model],\n",
    "    test_data: Input[Dataset],\n",
    "    eval_slices_metrics: Output[Metrics],\n",
    "    gcs_artifact_path: str,\n",
    "    f1_threshold: float = 0.6,\n",
    "):\n",
    "    import logging\n",
    "    import json\n",
    "    import importlib.util\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import sys\n",
    "    from arsa_pipeline_tools.utils import download_files_from_gcs, load_module_from_file\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    download_files_from_gcs(code_bucket_path,code_dir,ALLOWED_EXTENSIONS)\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    # Ensure `evaluate_module_slices.py` exists\n",
    "    evaluate_script_path = os.path.join(code_dir, \"evaluate_model_slices.py\")\n",
    "    if not os.path.exists(evaluate_script_path):\n",
    "        raise FileNotFoundError(f\"`evaluate_module_slices.py` not found in {code_dir}\")\n",
    "\n",
    "    # Load `evaluate_module_slices.py` dynamically\n",
    "    evaluate_module_slices = load_module_from_file(evaluate_script_path)\n",
    "\n",
    "    logger.info(f\"model_gcs_path : {model_gcs_path},\\t model_gcs_path.uri {model_gcs_path.uri}, metadata {model_gcs_path.metadata['gcs_path']}\")\n",
    "    # Call `gcp_eval` method from the module\n",
    "    metrics_df = evaluate_module_slices.gcp_eval_slices(\n",
    "        test_df=test_data,\n",
    "        model_path=model_gcs_path.metadata[\"gcs_path\"],\n",
    "    )\n",
    "    logger.info(metrics_df)\n",
    "\n",
    "    gcs_bucket_name = gcs_artifact_path.split('/')[2]\n",
    "    gcs_blob_path = '/'.join(gcs_artifact_path.split('/')[3:])\n",
    "    csv_filename = f\"{gcs_blob_path}/slice_metrics.csv\"\n",
    "    json_filename = f\"{gcs_blob_path}/slice_metrics.json\"\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(gcs_bucket_name)\n",
    "\n",
    "    # Save as CSV\n",
    "    csv_blob = bucket.blob(csv_filename)\n",
    "    csv_blob.upload_from_string(metrics_df.to_csv(index=False), content_type=\"text/csv\")\n",
    "    logger.info(f\"Slice metrics saved to GCS as CSV at gs://{gcs_bucket_name}/{csv_filename}\")\n",
    "\n",
    "    # Save as JSON\n",
    "    json_blob = bucket.blob(json_filename)\n",
    "    json_blob.upload_from_string(metrics_df.to_json(orient=\"records\"), content_type=\"application/json\")\n",
    "    logger.info(f\"Slice metrics saved to GCS as JSON at gs://{gcs_bucket_name}/{json_filename}\")\n",
    "\n",
    "    # Log paths of the artifacts in metrics\n",
    "    eval_slices_metrics.metadata[\"slice_metrics_csv\"] = f\"gs://{gcs_bucket_name}/{csv_filename}\"\n",
    "    eval_slices_metrics.metadata[\"slice_metrics_json\"] = f\"gs://{gcs_bucket_name}/{json_filename}\"\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"torch\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "        \"transformers[torch]\",  # Compatible with PyTorch 1.12\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"gcsfs\",\n",
    "        \"arsa-pipeline-tools\",\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def bias_detect_component(\n",
    "    code_bucket_path: str,\n",
    "    metrics: Input[Metrics],\n",
    "    gcs_artifact_path: str,\n",
    ")-> NamedTuple(\"output\", [(\"bias_detect\", str)]):\n",
    "    import logging\n",
    "    import json\n",
    "    import importlib.util\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import sys\n",
    "    from arsa_pipeline_tools.utils import download_files_from_gcs, load_module_from_file\n",
    "\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Download code from GCS\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "    \n",
    "    download_files_from_gcs(code_bucket_path,code_dir,ALLOWED_EXTENSIONS)\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    # Ensure `evaluate_module_slices.py` exists\n",
    "    evaluate_script_path = os.path.join(code_dir, \"bias_detect.py\")\n",
    "    if not os.path.exists(evaluate_script_path):\n",
    "        raise FileNotFoundError(f\"`evaluate_module_slices.py` not found in {code_dir}\")\n",
    "\n",
    "    # Load `evaluate_module_slices.py` dynamically\n",
    "    bias_detect = load_module_from_file(evaluate_script_path)\n",
    "\n",
    "\n",
    "    # Call `gcp_eval` method from the module\n",
    "    biased_rows, f1_threshold = bias_detect.detect_bias(\n",
    "        slice_metrics_path=metrics.metadata[\"slice_metrics_csv\"],\n",
    "    )\n",
    "    bias_report = {}\n",
    "\n",
    "    # Log results\n",
    "    if not biased_rows.empty:\n",
    "        bias_report[\"bias_detected\"] = True\n",
    "        bias_report[\"details\"] = biased_rows.to_dict(orient=\"records\")\n",
    "\n",
    "        logger.warning(\"Potential bias detected in the following slices:\")\n",
    "        for _, row in biased_rows.iterrows():\n",
    "            logger.error(\n",
    "                f\"Slice Column: {row['Slice Column']}, Slice Value: {row['Slice Value']}, \"\n",
    "                f\"Samples: {row['Samples']}, F1 Score: {row['F1 Score']:.4f} (Threshold: {f1_threshold:.4f})\"\n",
    "            )\n",
    "        logger.error(\"Potential bias detected. Check bias_detection.log for details.\")\n",
    "    else:\n",
    "        bias_report[\"bias_detected\"] = False\n",
    "        bias_report[\"details\"] = []\n",
    "        logger.info(\"No significant bias detected.\")\n",
    "        # print(\"No significant bias detected.\")\n",
    "\n",
    "    # Save bias report as JSON to GCS\n",
    "    gcs_bucket_name = gcs_artifact_path.split('/')[2]\n",
    "    gcs_blob_path = '/'.join(gcs_artifact_path.split('/')[3:])\n",
    "    bias_json_path = f\"{gcs_blob_path}/bias.json\"\n",
    "\n",
    "    try:\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(gcs_bucket_name)\n",
    "        blob = bucket.blob(bias_json_path)\n",
    "        blob.upload_from_string(json.dumps(bias_report, indent=4), content_type=\"application/json\")\n",
    "        logging.info(f\"Bias report saved to GCS at gs://{gcs_bucket_name}/{bias_json_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save bias report to GCS: {e}\")\n",
    "        raise\n",
    "\n",
    "    # bias_metrics.log_metric(\"bias_detected\",)\n",
    "    \n",
    "    # Raise an error if bias is detected to stop the pipeline\n",
    "    if bias_report[\"bias_detected\"]:\n",
    "        # bias_metrics.log_metric(\"bias_detected\",True)\n",
    "        bias_detect = \"true\"\n",
    "        return (bias_detect,)\n",
    "        # raise RuntimeError(\"Bias detected in slice metrics. Stopping the pipeline.\")\n",
    "    else:\n",
    "        bias_detect = \"false\"\n",
    "        return (bias_detect,)\n",
    "        # bias_metrics.log_metric(\"bias_detected\",False)\n",
    "\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage\", \"google-cloud-build\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def build_and_push_torchserve_image(\n",
    "    code_bucket_path: str,\n",
    "    gcp_project: str, \n",
    "    gcp_region: str, \n",
    "    bucket_name: str, \n",
    "    docker_image_name: str,\n",
    "    model_gcs_path: Input[Model]\n",
    "):\n",
    "    # Import inside the component\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import os\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Define environment variables\n",
    "    TORCH_SERVE_PATH = f\"gs://{bucket_name}/code/predictor/\"\n",
    "    CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{gcp_project}/{docker_image_name}\"\n",
    "\n",
    "    # Set up the CloudBuild client\n",
    "    client = cloudbuild.CloudBuildClient()\n",
    "\n",
    "    # Log the environment variables for debugging\n",
    "    logger.info(f\"GCP Project: {gcp_project}\")\n",
    "    logger.info(f\"GCP Region: {gcp_region}\")\n",
    "    logger.info(f\"Bucket Name: {bucket_name}\")\n",
    "    logger.info(f\"TorchServe Path: {TORCH_SERVE_PATH}\")\n",
    "    logger.info(f\"Docker Image Name: {docker_image_name}\")\n",
    "    logger.info(f\"Custom Docker Image URI: {CUSTOM_PREDICTOR_IMAGE_URI}\")\n",
    "        \n",
    "    model_gcs_path = model_gcs_path.metadata[\"gcs_path\"]\n",
    "    logger.info(model_gcs_path)\n",
    "    model_gcs_path = f\"gs://{bucket_name}/output/models/\"\n",
    "    # Create Cloud Build configuration (cloudbuild.yaml)\n",
    "    cloudbuild_config = {\n",
    "        'steps': [\n",
    "            # Step 1: Download code files from GCS\n",
    "            {\n",
    "                \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "                \"args\": [\n",
    "                    'cp',\n",
    "                    '-r',  # Recursive copy\n",
    "                    f'{code_bucket_path}/*',  # Copy all contents from the code folder\n",
    "                    '.'  # Copy to the current working directory\n",
    "                ],\n",
    "            },\n",
    "            # Step 2: Create the destination directory for model files\n",
    "            {\n",
    "                \"name\": \"ubuntu\",\n",
    "                \"args\": [\n",
    "                    \"mkdir\",\n",
    "                    \"-p\",  # Create parent directories as needed\n",
    "                    \"./bert-sent-model\"\n",
    "                ],\n",
    "            },\n",
    "            # Step 3: Download model files from GCS\n",
    "            {\n",
    "                \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "                \"args\": [\n",
    "                    'cp',\n",
    "                    '-r',  # Recursive copy\n",
    "                    f'{model_gcs_path}*',  # Add wildcard to include all files in the folder\n",
    "                    './bert-sent-model/'  # Ensure the trailing slash\n",
    "                ],\n",
    "            },\n",
    "            # Step 3: List files in the current working directory\n",
    "            {\n",
    "                \"name\": \"ubuntu\",\n",
    "                \"args\": [\n",
    "                    \"ls\",\n",
    "                    \"-R\",  # Recursive listing\n",
    "                    \".\"    # Current working directory\n",
    "                ],\n",
    "            },\n",
    "            # Step 4: Build the Docker image\n",
    "            {\n",
    "                'name': 'gcr.io/cloud-builders/docker',\n",
    "                'args': [\n",
    "                    'build',\n",
    "                    '-t',\n",
    "                    CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "                    '.'\n",
    "                ],\n",
    "            },\n",
    "            # Step 5: Push the Docker image to the container registry\n",
    "            {\n",
    "                'name': 'gcr.io/cloud-builders/docker',\n",
    "                'args': [\n",
    "                    'push',\n",
    "                    CUSTOM_PREDICTOR_IMAGE_URI\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        'images': [CUSTOM_PREDICTOR_IMAGE_URI],\n",
    "    }\n",
    "\n",
    "    # Create a Cloud Build build request\n",
    "    build = cloudbuild.Build(\n",
    "        steps=cloudbuild_config['steps'],\n",
    "        images=cloudbuild_config['images'],\n",
    "    )\n",
    "\n",
    "    # Trigger Cloud Build job\n",
    "    build_response = client.create_build(project_id=gcp_project, build=build)\n",
    "\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(build_response.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = build_response.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"google-auth\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def upload_model_to_registry(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    model_display_name: str,\n",
    "    docker_image_uri: str,\n",
    "    model_description: str,\n",
    "    app_name: str,\n",
    "    health_route: str = \"/ping\",\n",
    "    predict_route: str = \"/predictions/\",\n",
    "    serving_container_ports: list = [7080],\n",
    ") -> NamedTuple(\"Outputs\", [(\"model_display_name\", str), (\"model_resource_name\", str), (\"model_version\", str)]):\n",
    "    \"\"\"Uploads the model to the AI platform and ensures versioning.\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    # Initialize the AI Platform\n",
    "    aiplatform.init(project=project_id, location=region, staging_bucket=bucket_name)\n",
    "\n",
    "    # Check if the model with the same display name exists\n",
    "    existing_models = aiplatform.Model.list(filter=f\"display_name={model_display_name}\")\n",
    "    \n",
    "    if existing_models:\n",
    "        # Model exists, register as a new version\n",
    "        model_resource_name = existing_models[0].resource_name\n",
    "        print(f\"Model with display name '{model_display_name}' exists. Registering as a new version.\")\n",
    "        model_version = f\"v{len(existing_models) + 1}\"  # Increment version number\n",
    "    else:\n",
    "        # Model does not exist, create a new one\n",
    "        model_resource_name = None\n",
    "        print(f\"Model with display name '{model_display_name}' does not exist. Creating a new model.\")\n",
    "        model_version = \"v1\"\n",
    "\n",
    "    # Upload the model\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        description=model_description,\n",
    "        serving_container_image_uri=docker_image_uri,\n",
    "        serving_container_predict_route=predict_route + app_name,\n",
    "        serving_container_health_route=health_route,\n",
    "        serving_container_ports=serving_container_ports,\n",
    "        parent_model=model_resource_name,  # Register under an existing model if applicable\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    # Return output information\n",
    "    return (model.display_name, model.resource_name, model_version)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"data-prep-and-train\",\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    ")\n",
    "def data_prep_and_train_pipeline():\n",
    "    # Step 1: Data Preparation\n",
    "    data_prep_task = data_prep_stage(\n",
    "        code_bucket_path=SOURCE_CODE,\n",
    "        input_path=DATA_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "    )\n",
    "    \n",
    "    optuna_experiment_task = run_optuna_experiment(\n",
    "        code_bucket_path=SOURCE_CODE,\n",
    "        data_path=DATA_PATH,\n",
    "        train_data=data_prep_task.outputs[\"train_data\"],\n",
    "        val_data=data_prep_task.outputs[\"val_data\"],\n",
    "        test_data=data_prep_task.outputs[\"test_data\"],\n",
    "    ).set_cpu_limit(\"8\") \\\n",
    "     .set_memory_limit(\"32G\") \\\n",
    "     .set_gpu_limit(1) \\\n",
    "     .set_accelerator_type(\"NVIDIA_TESLA_T4\")\n",
    "\n",
    "\n",
    "    # Step 2: Training and Saving Model\n",
    "    train_save_task = train_save_stage(\n",
    "        code_bucket_path=SOURCE_CODE,\n",
    "        data_path=OUTPUT_DIR,\n",
    "        model_save_path=MODEL_SAVE_PATH,\n",
    "        train_data=data_prep_task.outputs[\"train_data\"],\n",
    "        val_data=data_prep_task.outputs[\"val_data\"],\n",
    "        best_hyperparams_metrics=optuna_experiment_task.outputs[\"best_hyperparams_metrics\"],\n",
    "    ).set_cpu_limit(\"8\") \\\n",
    "     .set_memory_limit(\"32G\") \\\n",
    "     .set_gpu_limit(1) \\\n",
    "     .set_accelerator_type(\"NVIDIA_TESLA_T4\")\n",
    "\n",
    "    # Step 3: Model Evaluation\n",
    "    evaluate_task = evaluate_model_component(\n",
    "        code_bucket_path=SOURCE_CODE,\n",
    "        model_gcs_path=train_save_task.outputs[\"model\"],  # Pass Model artifact\n",
    "        test_data=data_prep_task.outputs[\"test_data\"],  # Pass Test Data artifact\n",
    "        f1_threshold=0.6,\n",
    "    )\n",
    "\n",
    "    optuna_experiment_task.after(data_prep_task)\n",
    "    train_save_task.after(optuna_experiment_task)\n",
    "    \n",
    "    # Conditional Logic: Check if eval passed\n",
    "    with dsl.If(evaluate_task.outputs[\"eval_pass\"] == \"true\", name=\"conditional-validation-check\"):\n",
    "        # Step 4: Evaluate Slices\n",
    "        evaluate_slices_task = evaluate_slices_component(\n",
    "            code_bucket_path=SOURCE_CODE,\n",
    "            model_gcs_path=train_save_task.outputs[\"model\"], \n",
    "            test_data=data_prep_task.outputs[\"test_data\"],  \n",
    "            gcs_artifact_path=SLICE_METRIC_PATH,\n",
    "            f1_threshold=0.6,\n",
    "        )\n",
    "        \n",
    "        # Step 5: Bias Detection\n",
    "        bias_detect_task = bias_detect_component(\n",
    "            code_bucket_path=SOURCE_CODE,\n",
    "            metrics=evaluate_slices_task.outputs[\"eval_slices_metrics\"],\n",
    "            gcs_artifact_path=SLICE_METRIC_PATH,\n",
    "        )\n",
    "        evaluate_slices_task.after(evaluate_task)\n",
    "        bias_detect_task.after(evaluate_slices_task)\n",
    "\n",
    "        with dsl.If(bias_detect_task.outputs[\"bias_detect\"] == \"false\", name=\"bias-check-condtional-deploy\"):\n",
    "\n",
    "            # Step 6: Build and Push TorchServe Image\n",
    "            build_and_push_torchserve_image_op = build_and_push_torchserve_image(\n",
    "                code_bucket_path=SOURCE_CODE, \n",
    "                gcp_project=GCP_PROJECT,\n",
    "                gcp_region=GCP_REGION,\n",
    "                bucket_name=BUCKET_NAME,\n",
    "                docker_image_name=\"pytorch_predict_review_sentiment_bert_model\",\n",
    "                model_gcs_path=train_save_task.outputs[\"model\"],\n",
    "            )\n",
    "        # Task dependencies within the successful branch\n",
    "\n",
    "\n",
    "            upload_model_task = upload_model_to_registry(\n",
    "                project_id=PROJECT_ID,\n",
    "                region=GCP_REGION,\n",
    "                bucket_name=BUCKET_NAME,\n",
    "                model_display_name=MODEL_DISPLAY_NAME,\n",
    "                docker_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "                model_description=MODEL_DESCRIPTION,\n",
    "                app_name=APP_NAME,\n",
    "            )\n",
    "            upload_model_task.set_caching_options(False)\n",
    "            build_and_push_torchserve_image_op.set_caching_options(False)\n",
    "            build_and_push_torchserve_image_op.after(bias_detect_task)\n",
    "            upload_model_task.after(build_and_push_torchserve_image_op)\n",
    "\n",
    "    # with dsl.If(evaluate_task.outputs[\"eval_pass\"] == \"false\", name=\"conditional-validation-check\"):\n",
    "    #     train_save_task.after(evaluate_task)\n",
    "\n",
    "from kfp.v2.compiler import Compiler\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Define the pipeline file path\n",
    "pipeline_file_path = \"data_prep_and_train_pipeline.json\"\n",
    "\n",
    "# Compile the pipeline\n",
    "Compiler().compile(pipeline_func=data_prep_and_train_pipeline, package_path=pipeline_file_path)\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=GCP_PROJECT, location=GCP_REGION)\n",
    "\n",
    "# Submit the pipeline to Vertex AI\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"data-prep-and-train-pipeline\",\n",
    "    template_path=pipeline_file_path,\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    ")\n",
    "\n",
    "pipeline_job.submit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI = gcr.io/amazonreviewssentimentanalysis/pytorch_predict_review_sentiment_bert_model\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
    "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/0)  docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                    docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (12/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (14/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m => [11/17] RUN ls -l /home/model-server                                   0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (14/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[2m => => # -rw-rw-r-- 1 root         root  309 Sep 30 21:38 config.properties    \n",
      "\u001b[0m\u001b[2m => => # -rw-rw-r-- 1 root         root 7363 Dec  2 23:43 custom_handler.py    \n",
      "\u001b[0m\u001b[2m => => # -rw-rw-r-- 1 root         root   66 Nov 30 20:07 index_to_name.json   \n",
      "\u001b[0m\u001b[2m => => # drwxr-xr-x 2 model-server root 4096 Sep 30 21:41 model-store          \n",
      "\u001b[0m\u001b[2m => => # drwxr-xr-x 1 model-server root 4096 Nov 25 17:35 tmp                  \n",
      "\u001b[0m\u001b[2m => => # drwxr-xr-x 3 root         root 4096 Nov 29 20:05 utils                \n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (16/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "\u001b[3A\u001b[0G\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.3s (17/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (17/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (18/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m => [16/17] RUN chown -R model-server:model-server /home/model-server      0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.9s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m => [16/17] RUN chown -R model-server:model-server /home/model-server      0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.0s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m => [16/17] RUN chown -R model-server:model-server /home/model-server      0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.2s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m => [16/17] RUN chown -R model-server:model-server /home/model-server      0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.3s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m => [16/17] RUN chown -R model-server:model-server /home/model-server      0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.5s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m => [16/17] RUN chown -R model-server:model-server /home/model-server      0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.6s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.8s (19/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.8s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.0s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.1s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.3s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.4s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.6s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.7s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.9s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  1.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.0s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.2s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  1.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.3s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.5s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  1.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.6s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.8s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  2.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.9s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  2.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.1s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  2.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.2s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  2.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.4s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  2.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.5s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  2.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.7s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  2.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.8s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  3.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.0s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  3.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.1s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  3.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.3s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  3.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.4s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  3.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.6s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  3.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.7s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  3.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 6.9s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  4.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.0s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  4.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.2s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  4.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.3s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  4.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.5s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  4.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.6s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  4.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.8s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  5.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 7.9s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  5.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.1s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  5.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.2s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  5.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.4s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  5.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.5s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  5.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.7s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  5.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 8.8s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  6.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.0s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  6.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.1s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  6.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.3s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  6.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.4s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  6.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.6s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  6.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.7s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  6.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 9.9s (20/21)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  7.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.0s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  7.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.2s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  7.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.3s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  7.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.5s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  7.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.6s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  7.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.8s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  8.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 10.9s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  8.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.1s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  8.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.2s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  8.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.4s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  8.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.5s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  8.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.7s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  8.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 11.8s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  9.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.0s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  9.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.1s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  9.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.3s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  9.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.4s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  9.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.6s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  9.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.7s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentiment  9.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 12.9s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  10.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.0s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  10.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.2s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  10.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.3s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  10.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.5s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  10.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.6s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  10.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.8s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  11.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 13.9s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  11.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.1s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  11.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.2s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  11.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.4s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  11.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.5s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  11.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.7s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  11.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 14.8s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  12.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.0s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  12.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.1s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  12.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.3s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  12.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.4s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  12.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.6s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  12.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.7s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  12.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 15.9s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  13.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.0s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  13.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.2s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  13.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.3s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  13.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.5s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  13.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.6s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  13.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.8s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 16.9s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.1s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.2s (20/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.2s (21/21)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m\u001b[34m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.4s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.4s (21/22)                                 docker:desktop-linux\n",
      "\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m\u001b[34m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.4s\n",
      "\u001b[0m => exporting to image                                                     0.2s\n",
      " => => exporting layers                                                    0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.5s (21/22)                                 docker:desktop-linux\n",
      "\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m\u001b[34m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.4s\n",
      "\u001b[0m => exporting to image                                                     0.3s\n",
      " => => exporting layers                                                    0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.7s (21/22)                                 docker:desktop-linux\n",
      "\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m\u001b[34m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.4s\n",
      "\u001b[0m => exporting to image                                                     0.5s\n",
      " => => exporting layers                                                    0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.8s (21/22)                                 docker:desktop-linux\n",
      "\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m\u001b[34m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.4s\n",
      "\u001b[0m => exporting to image                                                     0.6s\n",
      " => => exporting layers                                                    0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.8s (21/22)                                 docker:desktop-linux\n",
      "\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m\u001b[34m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.4s\n",
      "\u001b[0m => exporting to image                                                     0.6s\n",
      "\u001b[34m => => exporting layers                                                    0.6s\n",
      "\u001b[0m => => writing image sha256:cb12cbac6a524f22a2ef65f79c193401b3782c856ed20  0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 17.9s (22/22) FINISHED                        docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.33kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/torchserve:latest-cpu   0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 8.78kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/17] FROM docker.io/pytorch/torchserve:latest-cpu@sha256:50e189492  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/17] RUN python3 -m pip install --upgrade pip                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/17] RUN pip3 install transformers                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/17] RUN ls -l                                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/17] WORKDIR /home/model-server/                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/17] COPY ./bert-sent-model /home/model-server/bert-sent-mo  0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/17] COPY ./custom_handler.py /home/model-server/custom_handler.py  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/17] COPY ./bert_model_torch_serve.py /home/model-server/bert_mode  0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/17] COPY ./utils /home/model-server/utils/                         0.0s\n",
      "\u001b[0m\u001b[34m => [10/17] COPY ./index_to_name.json /home/model-server/                  0.0s\n",
      "\u001b[0m\u001b[34m => [11/17] RUN ls -l /home/model-server                                   0.3s\n",
      "\u001b[0m\u001b[34m => [12/17] RUN printf \"\\nservice_envelope=json\" >> /home/model-server/co  0.1s\n",
      "\u001b[0m\u001b[34m => [13/17] RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home  0.1s\n",
      "\u001b[0m\u001b[34m => [14/17] RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /hom  0.2s\n",
      "\u001b[0m\u001b[34m => [15/17] RUN printf \"\\ndisable_token_authorization=true\" >> /home/mode  0.1s\n",
      "\u001b[0m\u001b[34m => [16/17] RUN chown -R model-server:model-server /home/model-server      1.3s\n",
      "\u001b[0m\u001b[34m => [17/17] RUN torch-model-archiver -f     --model-name=review_sentimen  14.4s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.6s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.6s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:cb12cbac6a524f22a2ef65f79c193401b3782c856ed20  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to gcr.io/amazonreviewssentimentanalysis/pytorch_predict_re  0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "View build details: \u001b]8;;docker-desktop://dashboard/build/desktop-linux/desktop-linux/bf2d21x5i0dwikde4nerkfk38\u001b\\docker-desktop://dashboard/build/desktop-linux/desktop-linux/bf2d21x5i0dwikde4nerkfk38\u001b]8;;\u001b\\\n",
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    View a summary of image vulnerabilities and recommendations → \u001b[36mdocker scout quickview \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI \\\n",
    "  ./src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_bert_classifier\n",
      "711befb57237864ebe19402caf522bb01e4a2024f951677f56134a2c6476bce7\n"
     ]
    }
   ],
   "source": [
    "!docker stop local_bert_classifier\n",
    "!docker run -t -d --rm -p 7080:7080 --name=local_bert_classifier $CUSTOM_PREDICTOR_IMAGE_URI\n",
    "!sleep 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 List of found errors:\t1.Field: batch_prediction_job.input_config.bigquery_source; Message: Invalid BigQuery source.\t [field_violations {\n  field: \"batch_prediction_job.input_config.bigquery_source\"\n  description: \"Invalid BigQuery source.\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"List of found errors:\t1.Field: batch_prediction_job.input_config.bigquery_source; Message: Invalid BigQuery source.\t\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2607:f8b0:4006:816::200a%5D:443 {created_time:\"2024-12-02T22:41:59.431126647-05:00\", grpc_status:3, grpc_message:\"List of found errors:\\t1.Field: batch_prediction_job.input_config.bigquery_source; Message: Invalid BigQuery source.\\t\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m input_table_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbq://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_table_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Execute the query and pass the result to Vertex AI for batch prediction\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m batch_prediction_job \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_prediction_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbigquery_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_table_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Source is filtered by the BigQuery query\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbigquery_destination_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_table_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn1-standard-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstarting_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_parameters={\"include_columns\": \"text,title,price,price_missing,helpful_vote,verified_purchase\"}  # If needed for model-specific requirements\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# # Monitor the batch prediction job\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(\"Waiting for the batch prediction job to complete...\")\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# batch_prediction_job.wait()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#     print(f\"Batch prediction job failed. Error: {batch_prediction_job.error_message}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/cloud/aiplatform/models.py:5800\u001b[0m, in \u001b[0;36mModel.batch_predict\u001b[0;34m(self, job_display_name, gcs_source, bigquery_source, instances_format, gcs_destination_prefix, bigquery_destination_prefix, predictions_format, model_parameters, machine_type, accelerator_type, accelerator_count, starting_replica_count, max_replica_count, generate_explanation, explanation_metadata, explanation_parameters, labels, credentials, encryption_spec_key_name, sync, create_request_timeout, batch_size, service_account)\u001b[0m\n\u001b[1;32m   5603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_predict\u001b[39m(\n\u001b[1;32m   5604\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5605\u001b[0m     job_display_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5629\u001b[0m     service_account: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5630\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jobs\u001b[38;5;241m.\u001b[39mBatchPredictionJob:\n\u001b[1;32m   5631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a batch prediction job using this Model and outputs\u001b[39;00m\n\u001b[1;32m   5632\u001b[0m \u001b[38;5;124;03m    prediction results to the provided destination prefix in the specified\u001b[39;00m\n\u001b[1;32m   5633\u001b[0m \u001b[38;5;124;03m    `predictions_format`. One source and one destination prefix are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5797\u001b[0m \u001b[38;5;124;03m            Instantiated representation of the created batch prediction job.\u001b[39;00m\n\u001b[1;32m   5798\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchPredictionJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5803\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgcs_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcs_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbigquery_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgcs_destination_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcs_destination_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbigquery_destination_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_destination_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5811\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5812\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstarting_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerate_explanation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_explanation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplanation_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplanation_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencryption_spec_key_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_spec_key_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5824\u001b[0m \u001b[43m        \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/cloud/aiplatform/jobs.py:620\u001b[0m, in \u001b[0;36mBatchPredictionJob.create\u001b[0;34m(cls, job_display_name, model_name, instances_format, predictions_format, gcs_source, bigquery_source, gcs_destination_prefix, bigquery_destination_prefix, model_parameters, machine_type, accelerator_type, accelerator_count, starting_replica_count, max_replica_count, generate_explanation, explanation_metadata, explanation_parameters, labels, project, location, credentials, encryption_spec_key_name, sync, create_request_timeout, batch_size, model_monitoring_objective_config, model_monitoring_alert_config, analysis_instance_schema_uri, service_account)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m     service_account: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatchPredictionJob\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a batch prediction job.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m            Instantiated representation of the created batch prediction job.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgcs_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcs_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbigquery_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgcs_destination_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcs_destination_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbigquery_destination_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_destination_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstarting_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerate_explanation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_explanation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplanation_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplanation_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencryption_spec_key_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_spec_key_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_monitoring_objective_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_monitoring_objective_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_monitoring_alert_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_monitoring_alert_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43manalysis_instance_schema_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalysis_instance_schema_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Main distinction of `create` vs `submit`:\u001b[39;49;00m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_completion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/cloud/aiplatform/jobs.py:1337\u001b[0m, in \u001b[0;36mBatchPredictionJob._submit_impl\u001b[0;34m(cls, job_display_name, model_name, instances_format, predictions_format, gcs_source, bigquery_source, gcs_destination_prefix, bigquery_destination_prefix, model_parameters, machine_type, accelerator_type, accelerator_count, starting_replica_count, max_replica_count, generate_explanation, explanation_metadata, explanation_parameters, labels, project, location, credentials, encryption_spec_key_name, sync, create_request_timeout, batch_size, model_monitoring_objective_config, model_monitoring_alert_config, analysis_instance_schema_uri, service_account, wait_for_completion)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     gapic_batch_prediction_job\u001b[38;5;241m.\u001b[39mmodel_monitoring_config \u001b[38;5;241m=\u001b[39m gapic_mm_config\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;66;03m# TODO(b/242108750): remove temporary logic once model monitoring for batch prediction is GA\u001b[39;00m\n\u001b[0;32m-> 1337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit_and_optionally_wait_with_sync_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mempty_batch_prediction_job\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_batch_prediction_job\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_or_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgca_batch_prediction_job\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgapic_batch_prediction_job\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_explanation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_explanation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait_for_completion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/cloud/aiplatform/jobs.py:1406\u001b[0m, in \u001b[0;36mBatchPredictionJob._submit_and_optionally_wait_with_sync_support\u001b[0;34m(cls, empty_batch_prediction_job, model_or_model_name, gca_batch_prediction_job, generate_explanation, sync, create_request_timeout, wait_for_completion)\u001b[0m\n\u001b[1;32m   1402\u001b[0m api_client \u001b[38;5;241m=\u001b[39m empty_batch_prediction_job\u001b[38;5;241m.\u001b[39mapi_client\n\u001b[1;32m   1404\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_create_with_lro(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m-> 1406\u001b[0m gca_batch_prediction_job \u001b[38;5;241m=\u001b[39m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_batch_prediction_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_prediction_job\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgca_batch_prediction_job\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m empty_batch_prediction_job\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m gca_batch_prediction_job\n\u001b[1;32m   1414\u001b[0m batch_prediction_job \u001b[38;5;241m=\u001b[39m empty_batch_prediction_job\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/cloud/aiplatform_v1/services/job_service/client.py:3744\u001b[0m, in \u001b[0;36mJobServiceClient.create_batch_prediction_job\u001b[0;34m(self, request, parent, batch_prediction_job, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   3741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   3743\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 3744\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3751\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   3752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 List of found errors:\t1.Field: batch_prediction_job.input_config.bigquery_source; Message: Invalid BigQuery source.\t [field_violations {\n  field: \"batch_prediction_job.input_config.bigquery_source\"\n  description: \"Invalid BigQuery source.\"\n}\n]"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI client\n",
    "aiplatform.init(project=\"amazonreviewssentimentanalysis\", location=\"us-central1\")\n",
    "\n",
    "# Define the model name and input/output details\n",
    "model_name = \"projects/amazonreviewssentimentanalysis/locations/us-central1/models/7777458171236319232@1\"\n",
    "input_table = \"bq://amazonreviewssentimentanalysis.amazon_reviews_sentiment.processed_batch_data\"\n",
    "output_table_prefix = \"bq://amazonreviewssentimentanalysis.amazon_reviews_sentiment.batch_predictions\"\n",
    "batch_prediction_name = \"Amazon-Month-Batch-BQ\"\n",
    "\n",
    "# Define the columns to be included in the prediction\n",
    "include_columns = \"text,title,price,price_missing,helpful_vote,verified_purchase\"\n",
    "\n",
    "# Initialize the model using the model name\n",
    "model = aiplatform.Model(model_name=model_name)\n",
    "\n",
    "try:\n",
    "    # Configure and run the batch prediction job\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=batch_prediction_name,\n",
    "        bigquery_source=input_table,\n",
    "        bigquery_destination_prefix=output_table_prefix,\n",
    "        machine_type=\"n1-standard-2\",\n",
    "        starting_replica_count=2,\n",
    "        max_replica_count=2,\n",
    "        model_parameters={\"include_columns\": include_columns},\n",
    "        sync=True  # Set to False if you want the job to run asynchronously\n",
    "    )\n",
    "\n",
    "    # Print job completion message\n",
    "    print(f\"Batch Prediction Job '{batch_prediction_name}' completed.\")\n",
    "    print(f\"Prediction results saved to: {batch_prediction_job.output_info.bigquery_output_table}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log any errors that occur\n",
    "    print(f\"An error occurred during batch prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'BatchPredictionJob' has no attribute 'InputDataConfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m include_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext,title,price,price_missing,helpful_vote,verified_purchase\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a Batch Prediction Job\u001b[39;00m\n\u001b[1;32m     16\u001b[0m job \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mBatchPredictionJob\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     17\u001b[0m     display_name\u001b[38;5;241m=\u001b[39mbatch_prediction_name,\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m---> 19\u001b[0m     input_data_config\u001b[38;5;241m=\u001b[39m\u001b[43maiplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchPredictionJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInputDataConfig\u001b[49m(\n\u001b[1;32m     20\u001b[0m         bigquery_source\u001b[38;5;241m=\u001b[39maiplatform\u001b[38;5;241m.\u001b[39mBatchPredictionJob\u001b[38;5;241m.\u001b[39mBigQuerySource(\n\u001b[1;32m     21\u001b[0m             input_uri\u001b[38;5;241m=\u001b[39minput_table\n\u001b[1;32m     22\u001b[0m         )\n\u001b[1;32m     23\u001b[0m     ),\n\u001b[1;32m     24\u001b[0m     output_data_config\u001b[38;5;241m=\u001b[39maiplatform\u001b[38;5;241m.\u001b[39mBatchPredictionJob\u001b[38;5;241m.\u001b[39mOutputDataConfig(\n\u001b[1;32m     25\u001b[0m         bigquery_destination\u001b[38;5;241m=\u001b[39maiplatform\u001b[38;5;241m.\u001b[39mBatchPredictionJob\u001b[38;5;241m.\u001b[39mBigQueryDestination(\n\u001b[1;32m     26\u001b[0m             output_uri\u001b[38;5;241m=\u001b[39moutput_table_prefix\n\u001b[1;32m     27\u001b[0m         )\n\u001b[1;32m     28\u001b[0m     ),\n\u001b[1;32m     29\u001b[0m     model_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_columns}\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Wait for the job to complete\u001b[39;00m\n\u001b[1;32m     33\u001b[0m job\u001b[38;5;241m.\u001b[39mwait()\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'BatchPredictionJob' has no attribute 'InputDataConfig'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline_utils.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ./pipeline_utils.py\n",
    "# def download_files_from_gcs(code_bucket_path: str, code_dir: str, allowed_extensions=None):\n",
    "#     \"\"\"\n",
    "#     Downloads files from a GCS bucket and saves them to a specified local directory.\n",
    "\n",
    "#     Args:\n",
    "#         code_bucket_path (str): The GCS bucket path (e.g., gs://bucket-name/folder).\n",
    "#         code_dir (str): The local directory where files will be downloaded.\n",
    "#         allowed_extensions (set): A set of allowed file extensions to filter downloads.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import logging\n",
    "#     from google.cloud import storage\n",
    "\n",
    "#     # Configure logging\n",
    "#     logging.basicConfig(level=logging.INFO)\n",
    "#     logger = logging.getLogger(__name__)\n",
    "\n",
    "#     # Parse bucket name and prefix from the GCS path\n",
    "#     bucket_name = code_bucket_path.split('/')[2]\n",
    "#     prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "\n",
    "#     # Initialize the GCS client\n",
    "#     client = storage.Client()\n",
    "#     bucket = client.bucket(bucket_name)\n",
    "#     blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "#     # Create the target directory\n",
    "#     os.makedirs(code_dir, exist_ok=True)\n",
    "\n",
    "#     # Set default allowed extensions\n",
    "#     if allowed_extensions is None:\n",
    "#         allowed_extensions = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "#     # Download files\n",
    "#     for blob in blobs:\n",
    "#         if any(blob.name.endswith(ext) for ext in allowed_extensions):\n",
    "#             relative_path = blob.name[len(prefix):].lstrip(\"/\")\n",
    "#             file_path = os.path.join(code_dir, relative_path)\n",
    "#             os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#             blob.download_to_filename(file_path)\n",
    "#             logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "#     logger.info(f\"All files downloaded to {code_dir}\")\n",
    "\n",
    "# def load_module_from_file(file_path):\n",
    "#     module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "#     spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "#     module = importlib.util.module_from_spec(spec)\n",
    "#     spec.loader.exec_module(module)\n",
    "#     return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kfp\n",
    "# from kfp.v2.dsl import pipeline, component\n",
    "# from kfp.v2 import compiler\n",
    "# from google.cloud import storage\n",
    "# import os\n",
    "\n",
    "# # Define the pipeline component\n",
    "# @component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-storage\"])\n",
    "# def gcs_transfer_component(\n",
    "#     source_bucket: str, destination_bucket: str, file_path_source: str, file_path_dest: str\n",
    "# ):\n",
    "#     \"\"\"Component to transfer files between GCS buckets.\"\"\"\n",
    "#     from google.cloud import storage  # Import inside component to avoid issues\n",
    "#     import os\n",
    "#     # Initialize GCS client\n",
    "#     storage_client = storage.Client()\n",
    "\n",
    "#     # Download the file from the source bucket\n",
    "#     source_blob = storage_client.bucket(source_bucket).blob(file_path_source)\n",
    "#     local_file = f\"/tmp/{os.path.basename(file_path_source)}\"\n",
    "#     source_blob.download_to_filename(local_file)\n",
    "#     print(f\"Downloaded {file_path_source} from bucket {source_bucket} to {local_file}\")\n",
    "\n",
    "#     # Upload the file to the destination bucket\n",
    "#     dest_blob = storage_client.bucket(destination_bucket).blob(f\"{file_path_dest}/{os.path.basename(file_path_source)}\")\n",
    "#     dest_blob.upload_from_filename(local_file)\n",
    "#     print(f\"Uploaded {file_path_source} to {destination_bucket}/{file_path_dest}\")\n",
    "\n",
    "# # Define the pipeline\n",
    "# @pipeline(name=\"gcs-transfer-pipeline\")\n",
    "# def gcs_pipeline(\n",
    "#     source_bucket: str = \"arsa_model_deployment_uscentral\",\n",
    "#     destination_bucket: str = \"arsa_model_deployment_uscentral_v2\",\n",
    "#     file_path_source: str = \"code/predictor/bert-sent-model/final_model.pth\",\n",
    "#     file_path_dest: str = \"output/models\",\n",
    "# ):\n",
    "#     # Add the GCS transfer component to the pipeline\n",
    "#     gcs_transfer_component(\n",
    "#         source_bucket=source_bucket,\n",
    "#         destination_bucket=destination_bucket,\n",
    "#         file_path_source=file_path_source,\n",
    "#         file_path_dest=file_path_dest,\n",
    "#     )\n",
    "\n",
    "# # Compile and run the pipeline from a notebook\n",
    "# from google.cloud import aiplatform\n",
    "\n",
    "# # Set GCP and Vertex AI configuration\n",
    "# PROJECT_ID = \"amazonreviewssentimentanalysis\"\n",
    "# REGION = \"us-central1\"\n",
    "# PIPELINE_NAME = \"gcs-transfer-pipeline\"\n",
    "# STAGING_BUCKET = \"gs://arsa_model_deployment_uscentral\"  # Fully qualified GCS bucket URI\n",
    "\n",
    "# # Compile the pipeline\n",
    "# pipeline_json = f\"{PIPELINE_NAME}.json\"\n",
    "# compiler.Compiler().compile(\n",
    "#     pipeline_func=gcs_pipeline,\n",
    "#     package_path=pipeline_json,\n",
    "# )\n",
    "\n",
    "# # Initialize the Vertex AI client\n",
    "# aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# # Submit the pipeline job\n",
    "# pipeline_job = aiplatform.PipelineJob(\n",
    "#     display_name=PIPELINE_NAME,\n",
    "#     template_path=pipeline_json,\n",
    "#     parameter_values={\n",
    "#         \"source_bucket\": \"arsa_model_deployment_uscentral\",\n",
    "#         \"destination_bucket\": \"arsa_model_deployment_uscentral_v2\",\n",
    "#         \"file_path_source\": \"code/predictor/bert-sent-model/final_model.pth\",\n",
    "#         \"file_path_dest\": \"output/models\",\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # Run the pipeline\n",
    "# pipeline_job.submit()\n",
    "\n",
    "# print(f\"Pipeline {PIPELINE_NAME} has been submitted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old pipeline\n",
    "\n",
    "# @dsl.pipeline(\n",
    "#     name=\"data-prep-and-train\",\n",
    "#     pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    "# )\n",
    "# def data_prep_and_train_pipeline():\n",
    "#     # Step 1: Data Preparation\n",
    "#     data_prep_task = data_prep_stage(\n",
    "#         code_bucket_path=SOURCE_CODE,\n",
    "#         input_path=DATA_PATH,\n",
    "#         output_dir=OUTPUT_DIR,\n",
    "#     )\n",
    "\n",
    "#     # Step 2: Training and Saving Model\n",
    "#     train_save_task = train_save_stage(\n",
    "#         code_bucket_path=SOURCE_CODE,\n",
    "#         data_path=OUTPUT_DIR,\n",
    "#         model_save_path=MODEL_SAVE_PATH,\n",
    "#         train_data=data_prep_task.outputs[\"train_data\"],\n",
    "#         val_data=data_prep_task.outputs[\"val_data\"],\n",
    "\n",
    "#     ).set_cpu_limit(\"8\") \\\n",
    "#      .set_memory_limit(\"32G\") \\\n",
    "#      .set_gpu_limit(1) \\\n",
    "#      .set_accelerator_type(\"NVIDIA_TESLA_T4\")\n",
    "\n",
    "#     evaluate_task = evaluate_model_component(\n",
    "#         code_bucket_path=SOURCE_CODE,\n",
    "#         model_gcs_path=train_save_task.outputs[\"model\"],  # Pass Model artifact\n",
    "#         test_data=data_prep_task.outputs[\"test_data\"],  # Pass Test Data artifact\n",
    "#         f1_threshold=0.6,\n",
    "#     )\n",
    "    \n",
    "    # evaluate_slices_task = evaluate_slices_component(\n",
    "    #     code_bucket_path=SOURCE_CODE,\n",
    "    #     model_gcs_path=train_save_task.outputs[\"model\"], \n",
    "    #     test_data=data_prep_task.outputs[\"test_data\"],  \n",
    "    #     gcs_artifact_path = SLICE_METRIC_PATH,\n",
    "    #     f1_threshold=0.6,\n",
    "    # )\n",
    "    # bias_detect_task = bias_detect_component(\n",
    "    #     code_bucket_path=SOURCE_CODE,\n",
    "    #     metrics=evaluate_slices_task.outputs[\"eval_slices_metrics\"],\n",
    "    #     gcs_artifact_path = SLICE_METRIC_PATH,\n",
    "    # )\n",
    "\n",
    "    # build_and_push_torchserve_image_op = build_and_push_torchserve_image(\n",
    "    #         code_bucket_path=SOURCE_CODE, \n",
    "    #         gcp_project=GCP_PROJECT,\n",
    "    #         gcp_region=GCP_REGION,\n",
    "    #         bucket_name=BUCKET_NAME,\n",
    "    #         docker_image_name=\"pytorch_predict_review_sentiment_bert_model\",\n",
    "    #         model_gcs_path=train_save_task.outputs[\"model\"],\n",
    "    #     )\n",
    "    \n",
    "    # train_save_task.after(data_prep_task)\n",
    "    # evaluate_task.after(train_save_task)\n",
    "\n",
    "    # evaluate_slices_task.after(evaluate_task)  \n",
    "    # bias_detect_task.after(evaluate_slices_task)\n",
    "    # build_and_push_torchserve_image_op.after(bias_detect_task)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
