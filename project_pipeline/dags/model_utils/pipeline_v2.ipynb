{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component, Input, Output, Dataset, Artifact\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from google.cloud import aiplatform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "GCP_PROJECT = \"amazonreviewssentimentanalysis\"\n",
    "GCP_REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"arsa_model_deployment_uscentral_v2\"\n",
    "DATA_PATH = f\"gs://{BUCKET_NAME}/input/labeled_data_1perc.csv\"\n",
    "OUTPUT_DIR = f\"gs://{BUCKET_NAME}/output/data/\"\n",
    "CODE_BUCKET_PATH = f\"gs://{BUCKET_NAME}/code\"\n",
    "# DATA_PREP_CODE = f\"gs://{BUCKET_NAME}/code/data_prep\"\n",
    "SOURCE_CODE = f\"gs://{BUCKET_NAME}/code/src\"\n",
    "SLICE_METRIC_PATH = f\"gs://{BUCKET_NAME}/output/metrics\"\n",
    "# TRAINER_CODE = f\"gs://{BUCKET_NAME}/code/trainer\"\n",
    "MODEL_SAVE_PATH = f\"gs://{BUCKET_NAME}/output/models/final_model.pth\"\n",
    "# TORCH_SERVE_PATH = f\"gs://{BUCKET_NAME}/code/predictor/\"\n",
    "VERSION = 1\n",
    "APP_NAME = \"review_sentiment_bert_model\"\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"{APP_NAME}-v{VERSION}\"\n",
    "MODEL_DESCRIPTION = \"PyTorch serve deploymend model for amazon reviews classification\"\n",
    "\n",
    "# MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{APP_NAME}\"\n",
    "serving_container_ports = [7080]\n",
    "\n",
    "PROJECT_ID = \"amazonreviewssentimentanalysis\" \n",
    "APP_NAME = \"review_sentiment_bert_model\"\n",
    "DOCKER_IMAGE_NAME = \"pytorch_predict_{APP_NAME}\"\n",
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google Cloud Storage client\n",
    "client = storage.Client(project=GCP_PROJECT)\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Function to upload folder to GCS\n",
    "def upload_folder_to_gcs(local_folder, bucket, destination_folder):\n",
    "    # Strip the `gs://<bucket_name>/` prefix from the destination path\n",
    "    if destination_folder.startswith(f\"gs://{bucket.name}/\"):\n",
    "        destination_folder = destination_folder[len(f\"gs://{bucket.name}/\"):]\n",
    "\n",
    "    for root, _, files in os.walk(local_folder):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_folder)\n",
    "            print(local_path,relative_path)\n",
    "\n",
    "            gcs_path = os.path.join(destination_folder, local_path).replace(\"\\\\\", \"/\")\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"Uploaded {local_path} to gs://{bucket.name}/{gcs_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/app.py app.py\n",
      "Uploaded src/app.py to gs://arsa_model_deployment_uscentral_v2/code/src/app.py\n",
      "src/app_requirements.txt app_requirements.txt\n",
      "Uploaded src/app_requirements.txt to gs://arsa_model_deployment_uscentral_v2/code/src/app_requirements.txt\n",
      "src/best_hyperparameters.json best_hyperparameters.json\n",
      "Uploaded src/best_hyperparameters.json to gs://arsa_model_deployment_uscentral_v2/code/src/best_hyperparameters.json\n",
      "src/bias_detect.py bias_detect.py\n",
      "Uploaded src/bias_detect.py to gs://arsa_model_deployment_uscentral_v2/code/src/bias_detect.py\n",
      "src/config.py config.py\n",
      "Uploaded src/config.py to gs://arsa_model_deployment_uscentral_v2/code/src/config.py\n",
      "src/custom_handler.py custom_handler.py\n",
      "Uploaded src/custom_handler.py to gs://arsa_model_deployment_uscentral_v2/code/src/custom_handler.py\n",
      "src/Dockerfile Dockerfile\n",
      "Uploaded src/Dockerfile to gs://arsa_model_deployment_uscentral_v2/code/src/Dockerfile\n",
      "src/evaluate_model.py evaluate_model.py\n",
      "Uploaded src/evaluate_model.py to gs://arsa_model_deployment_uscentral_v2/code/src/evaluate_model.py\n",
      "src/evaluate_model_slices.py evaluate_model_slices.py\n",
      "Uploaded src/evaluate_model_slices.py to gs://arsa_model_deployment_uscentral_v2/code/src/evaluate_model_slices.py\n",
      "src/experiment_runner.py experiment_runner.py\n",
      "Uploaded src/experiment_runner.py to gs://arsa_model_deployment_uscentral_v2/code/src/experiment_runner.py\n",
      "src/experiment_runner_optuna.py experiment_runner_optuna.py\n",
      "Uploaded src/experiment_runner_optuna.py to gs://arsa_model_deployment_uscentral_v2/code/src/experiment_runner_optuna.py\n",
      "src/index_to_name.json index_to_name.json\n",
      "Uploaded src/index_to_name.json to gs://arsa_model_deployment_uscentral_v2/code/src/index_to_name.json\n",
      "src/prepare_data.py prepare_data.py\n",
      "Uploaded src/prepare_data.py to gs://arsa_model_deployment_uscentral_v2/code/src/prepare_data.py\n",
      "src/README.md README.md\n",
      "Uploaded src/README.md to gs://arsa_model_deployment_uscentral_v2/code/src/README.md\n",
      "src/train_save.py train_save.py\n",
      "Uploaded src/train_save.py to gs://arsa_model_deployment_uscentral_v2/code/src/train_save.py\n",
      "src/data/label_encoder.pkl data/label_encoder.pkl\n",
      "Uploaded src/data/label_encoder.pkl to gs://arsa_model_deployment_uscentral_v2/code/src/data/label_encoder.pkl\n",
      "src/data/slice_metrics.csv data/slice_metrics.csv\n",
      "Uploaded src/data/slice_metrics.csv to gs://arsa_model_deployment_uscentral_v2/code/src/data/slice_metrics.csv\n",
      "src/data/test.pkl data/test.pkl\n",
      "Uploaded src/data/test.pkl to gs://arsa_model_deployment_uscentral_v2/code/src/data/test.pkl\n",
      "src/data/train.pkl data/train.pkl\n",
      "Uploaded src/data/train.pkl to gs://arsa_model_deployment_uscentral_v2/code/src/data/train.pkl\n",
      "src/data/val.pkl data/val.pkl\n",
      "Uploaded src/data/val.pkl to gs://arsa_model_deployment_uscentral_v2/code/src/data/val.pkl\n",
      "src/utils/bert_model.py utils/bert_model.py\n",
      "Uploaded src/utils/bert_model.py to gs://arsa_model_deployment_uscentral_v2/code/src/utils/bert_model.py\n",
      "src/utils/data_loader.py utils/data_loader.py\n",
      "Uploaded src/utils/data_loader.py to gs://arsa_model_deployment_uscentral_v2/code/src/utils/data_loader.py\n",
      "src/utils/roberta_model.py utils/roberta_model.py\n",
      "Uploaded src/utils/roberta_model.py to gs://arsa_model_deployment_uscentral_v2/code/src/utils/roberta_model.py\n",
      "src/utils/__init__.py utils/__init__.py\n",
      "Uploaded src/utils/__init__.py to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__init__.py\n",
      "src/utils/__pycache__/bert_model.cpython-312.pyc utils/__pycache__/bert_model.cpython-312.pyc\n",
      "Uploaded src/utils/__pycache__/bert_model.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__pycache__/bert_model.cpython-312.pyc\n",
      "src/utils/__pycache__/data_loader.cpython-312.pyc utils/__pycache__/data_loader.cpython-312.pyc\n",
      "Uploaded src/utils/__pycache__/data_loader.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__pycache__/data_loader.cpython-312.pyc\n",
      "src/utils/__pycache__/roberta_model.cpython-312.pyc utils/__pycache__/roberta_model.cpython-312.pyc\n",
      "Uploaded src/utils/__pycache__/roberta_model.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__pycache__/roberta_model.cpython-312.pyc\n",
      "src/utils/__pycache__/__init__.cpython-312.pyc utils/__pycache__/__init__.cpython-312.pyc\n",
      "Uploaded src/utils/__pycache__/__init__.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/utils/__pycache__/__init__.cpython-312.pyc\n",
      "src/__pycache__/config.cpython-312.pyc __pycache__/config.cpython-312.pyc\n",
      "Uploaded src/__pycache__/config.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/__pycache__/config.cpython-312.pyc\n",
      "src/__pycache__/evaluate_model.cpython-312.pyc __pycache__/evaluate_model.cpython-312.pyc\n",
      "Uploaded src/__pycache__/evaluate_model.cpython-312.pyc to gs://arsa_model_deployment_uscentral_v2/code/src/__pycache__/evaluate_model.cpython-312.pyc\n"
     ]
    }
   ],
   "source": [
    "upload_folder_to_gcs(\"src\", bucket, CODE_BUCKET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs/anaconda3/envs/mlops_pipeline/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/661148801406/locations/us-central1/pipelineJobs/data-prep-and-train-20241201130137\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/661148801406/locations/us-central1/pipelineJobs/data-prep-and-train-20241201130137')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/data-prep-and-train-20241201130137?project=661148801406\n",
      "PipelineJob projects/661148801406/locations/us-central1/pipelineJobs/data-prep-and-train-20241201130137 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/661148801406/locations/us-central1/pipelineJobs/data-prep-and-train-20241201130137\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    Metrics,\n",
    "    component,\n",
    "    pipeline,\n",
    ")\n",
    "from typing import NamedTuple\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"google-cloud-storage\", \"torch\", \"gcsfs\"],\n",
    ")\n",
    "def data_prep_stage(\n",
    "    code_bucket_path: str,\n",
    "    input_path: str,\n",
    "    output_dir: str,\n",
    "    train_data: Output[Dataset],\n",
    "    val_data: Output[Dataset],\n",
    "    test_data: Output[Dataset],\n",
    "\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    import importlib.util\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Logging setup\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Download code from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(code_bucket_path.split('/')[2])\n",
    "    prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    for blob in blobs:\n",
    "        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
    "            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n",
    "            file_path = os.path.join(code_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            blob.download_to_filename(file_path)\n",
    "            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    def load_module_from_file(file_path):\n",
    "        module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    prepare_data_module = load_module_from_file(f\"{code_dir}/prepare_data.py\")\n",
    "    train_df, val_df, test_df, label_encoder = prepare_data_module.split_and_save_data(input_path, output_dir)\n",
    "    train_df.to_pickle(train_data.path)\n",
    "    val_df.to_pickle(val_data.path)\n",
    "    test_df.to_pickle(test_data.path)\n",
    "    # label_encoder.to_pickle(label_encoder_data.path)\n",
    "    logger.info(\"Artifacts for train, dev, and test data created successfully.\")\n",
    "\n",
    "@component(\n",
    "    # packages_to_install=[\"torch\", \"google-cloud-storage\", \"transformers\", \"pandas\", \"scikit-learn\", \"gcsfs\",\"accelerate\"],\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"torch==1.12.1\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "        \"transformers==4.21.0\",  # Compatible with PyTorch 1.12\n",
    "        \"scikit-learn\",\n",
    "        \"accelerate==0.12.0\",  # Compatible with PyTorch 1.12 and transformers\n",
    "        \"google-cloud-storage\",\n",
    "        # \"kfp==2.0.0\",  # Compatible version of kfp\n",
    "        \"PyYAML>=6.0\",  # A stable version compatible with the other libraries\n",
    "        \"tensorboard\",\n",
    "    ],\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-12.py310:latest\",  # Python 3.10 with GPU support\n",
    ")\n",
    "def train_save_stage(\n",
    "    code_bucket_path: str,\n",
    "    data_path: str,\n",
    "    model_save_path: str,\n",
    "    train_data: Input[Dataset],\n",
    "    val_data: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    model_metrics: Output[Metrics],\n",
    "\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    import importlib.util\n",
    "    from accelerate import Accelerator\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Check available device\n",
    "    logger.info(f\"Using device: {accelerator.device}\")\n",
    "\n",
    "    # Download code from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(code_bucket_path.split('/')[2])\n",
    "    prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    for blob in blobs:\n",
    "        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
    "            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n",
    "            file_path = os.path.join(code_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            blob.download_to_filename(file_path)\n",
    "            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    def load_module_from_file(file_path):\n",
    "        module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    train_save_module = load_module_from_file(f\"{code_dir}/train_save.py\")\n",
    "    hyperparameters_path = os.path.join(code_dir, \"best_hyperparameters.json\")\n",
    "\n",
    "    returned_model_path, epoch_metrics = train_save_module.train_and_save_final_model(\n",
    "        hyperparameters=train_save_module.load_hyperparameters(hyperparameters_path),\n",
    "        data_path=data_path,\n",
    "        train_data = train_data,\n",
    "        val_data = val_data, \n",
    "        model_save_path=model_save_path,\n",
    "    )\n",
    "\n",
    "    # model_file\n",
    "    # = os.listdir(model_file)\n",
    "    # for file_name in model_file:\n",
    "    #     local_path = os.path.join(model_save_path, file_name)\n",
    "    #     blob_path = f\"output/models/{file_name}\"\n",
    "    #     blob = bucket.blob(blob_path)\n",
    "    #     blob.upload_from_filename(local_path)\n",
    "    #     logger.info(f\"Uploaded {local_path} to gs://{bucket.name}/{blob_path}\")\n",
    "\n",
    "    model.metadata[\"gcs_path\"] = returned_model_path\n",
    "    logger.info(f\"Model artifact metadata updated with GCS path: {returned_model_path}\")\n",
    "\n",
    "    print(epoch_metrics)\n",
    "    logger.info(f\"epoch_metrics: {epoch_metrics}\")\n",
    "    # Log metrics to the Vertex AI UI\n",
    "    # Corrected logging for Vertex AI\n",
    "    for epoch, metric in enumerate(epoch_metrics, start=1):\n",
    "        # Log accuracy and loss (ensure keys match)\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_accuracy\", metric[\"eval_accuracy\"])\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_loss\", metric[\"eval_loss\"])\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_precision\", metric[\"eval_precision\"])\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_recall\", metric[\"eval_recall\"])\n",
    "        model_metrics.log_metric(f\"epoch_{epoch}_f1\", metric[\"eval_f1\"])\n",
    "\n",
    "        # metrics.log_metric(f\"epoch_{epoch}_runtime\", metric[\"eval_runtime\"])\n",
    "        # metrics.log_metric(f\"epoch_{epoch}_samples_per_second\", metric[\"eval_samples_per_second\"])\n",
    "        # metrics.log_metric(f\"epoch_{epoch}_steps_per_second\", metric[\"eval_steps_per_second\"])\n",
    "        \n",
    "        # Log to standard output\n",
    "        logger.info(f\"Logged metrics for epoch {epoch}: {metric}\")\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"torch==1.12.1\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "        \"transformers==4.21.0\",  # Compatible with PyTorch 1.12\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"gcsfs\",\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def evaluate_model_component(\n",
    "    code_bucket_path: str,\n",
    "    model_gcs_path: Input[Model],\n",
    "    test_data: Input[Dataset],\n",
    "    eval_metrics: Output[Metrics],\n",
    "    # f1_score: Output[float],\n",
    "    f1_threshold: float = 0.6,\n",
    ")-> NamedTuple(\"output\", [(\"eval_pass\", str)]):\n",
    "    import logging\n",
    "    import json\n",
    "    import importlib.util\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Download code from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(code_bucket_path.split('/')[2])\n",
    "    prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    for blob in blobs:\n",
    "        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
    "            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n",
    "            file_path = os.path.join(code_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            blob.download_to_filename(file_path)\n",
    "            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    def load_module_from_file(file_path):\n",
    "        module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    # Ensure `evaluate_model.py` exists\n",
    "    evaluate_script_path = os.path.join(code_dir, \"evaluate_model.py\")\n",
    "    if not os.path.exists(evaluate_script_path):\n",
    "        raise FileNotFoundError(f\"`evaluate_model.py` not found in {code_dir}\")\n",
    "\n",
    "    # Load `evaluate_model.py` dynamically\n",
    "    evaluate_module = load_module_from_file(evaluate_script_path)\n",
    "\n",
    "    logger.info(f\"model_gcs_path : {model_gcs_path},\\t model_gcs_path.uri {model_gcs_path.uri}, metadata {model_gcs_path.metadata['gcs_path']}\")\n",
    "    # Call `gcp_eval` method from the module\n",
    "    accuracy, precision, recall, f1 = evaluate_module.gcp_eval(\n",
    "        test_df=test_data,\n",
    "        model_path=model_gcs_path.metadata[\"gcs_path\"],\n",
    "    )\n",
    "\n",
    "    # Log metrics to Vertex AI\n",
    "    eval_metrics.log_metric(\"accuracy\", accuracy)\n",
    "    eval_metrics.log_metric(\"precision\", precision)\n",
    "    eval_metrics.log_metric(\"recall\", recall)\n",
    "    eval_metrics.log_metric(\"f1\", f1)\n",
    "    # Conditional check\n",
    "    if f1 >= f1_threshold:\n",
    "        logger.info(f\"Model passed the F1 threshold: {f1:.4f} >= {f1_threshold}\")\n",
    "        eval_pass = \"true\"\n",
    "        return (eval_pass,)\n",
    "    else:\n",
    "        logger.error(f\"Model failed to meet the F1 threshold: {f1:.4f} < {f1_threshold}\")\n",
    "        eval_pass = \"false\"\n",
    "        return (eval_pass,)\n",
    "\n",
    "        # raise ValueError(f\"F1 score {f1:.4f} is below the threshold {f1_threshold}\")\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"torch==1.12.1\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "        \"transformers==4.21.0\",  # Compatible with PyTorch 1.12\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"gcsfs\",\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def evaluate_slices_component(\n",
    "    code_bucket_path: str,\n",
    "    model_gcs_path: Input[Model],\n",
    "    test_data: Input[Dataset],\n",
    "    eval_slices_metrics: Output[Metrics],\n",
    "    gcs_artifact_path: str,\n",
    "    f1_threshold: float = 0.6,\n",
    "):\n",
    "    import logging\n",
    "    import json\n",
    "    import importlib.util\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Download code from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(code_bucket_path.split('/')[2])\n",
    "    prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    for blob in blobs:\n",
    "        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
    "            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n",
    "            file_path = os.path.join(code_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            blob.download_to_filename(file_path)\n",
    "            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    def load_module_from_file(file_path):\n",
    "        module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    # Ensure `evaluate_module_slices.py` exists\n",
    "    evaluate_script_path = os.path.join(code_dir, \"evaluate_model_slices.py\")\n",
    "    if not os.path.exists(evaluate_script_path):\n",
    "        raise FileNotFoundError(f\"`evaluate_module_slices.py` not found in {code_dir}\")\n",
    "\n",
    "    # Load `evaluate_module_slices.py` dynamically\n",
    "    evaluate_module_slices = load_module_from_file(evaluate_script_path)\n",
    "\n",
    "    logger.info(f\"model_gcs_path : {model_gcs_path},\\t model_gcs_path.uri {model_gcs_path.uri}, metadata {model_gcs_path.metadata['gcs_path']}\")\n",
    "    # Call `gcp_eval` method from the module\n",
    "    metrics_df = evaluate_module_slices.gcp_eval_slices(\n",
    "        test_df=test_data,\n",
    "        model_path=model_gcs_path.metadata[\"gcs_path\"],\n",
    "    )\n",
    "    logger.info(metrics_df)\n",
    "\n",
    "    gcs_bucket_name = gcs_artifact_path.split('/')[2]\n",
    "    gcs_blob_path = '/'.join(gcs_artifact_path.split('/')[3:])\n",
    "    csv_filename = f\"{gcs_blob_path}/slice_metrics.csv\"\n",
    "    json_filename = f\"{gcs_blob_path}/slice_metrics.json\"\n",
    "\n",
    "    bucket = client.bucket(gcs_bucket_name)\n",
    "\n",
    "    # Save as CSV\n",
    "    csv_blob = bucket.blob(csv_filename)\n",
    "    csv_blob.upload_from_string(metrics_df.to_csv(index=False), content_type=\"text/csv\")\n",
    "    logger.info(f\"Slice metrics saved to GCS as CSV at gs://{gcs_bucket_name}/{csv_filename}\")\n",
    "\n",
    "    # Save as JSON\n",
    "    json_blob = bucket.blob(json_filename)\n",
    "    json_blob.upload_from_string(metrics_df.to_json(orient=\"records\"), content_type=\"application/json\")\n",
    "    logger.info(f\"Slice metrics saved to GCS as JSON at gs://{gcs_bucket_name}/{json_filename}\")\n",
    "\n",
    "    # Log paths of the artifacts in metrics\n",
    "    eval_slices_metrics.metadata[\"slice_metrics_csv\"] = f\"gs://{gcs_bucket_name}/{csv_filename}\"\n",
    "    eval_slices_metrics.metadata[\"slice_metrics_json\"] = f\"gs://{gcs_bucket_name}/{json_filename}\"\n",
    "    # # Log metrics to Vertex AI\n",
    "    # metrics.log_metric(\"accuracy\", accuracy)\n",
    "    # metrics.log_metric(\"precision\", precision)\n",
    "    # metrics.log_metric(\"recall\", recall)\n",
    "    # metrics.log_metric(\"f1\", f1)\n",
    "\n",
    "    # # Conditional check\n",
    "    # if f1 >= f1_threshold:\n",
    "    #     logger.info(f\"Model passed the F1 threshold: {f1:.4f} >= {f1_threshold}\")\n",
    "    # else:\n",
    "    #     logger.error(f\"Model failed to meet the F1 threshold: {f1:.4f} < {f1_threshold}\")\n",
    "    #     raise ValueError(f\"F1 score {f1:.4f} is below the threshold {f1_threshold}\")\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"torch==1.12.1\",  # PyTorch version 1.12.1, verified to work with transformers and accelerate\n",
    "        \"transformers==4.21.0\",  # Compatible with PyTorch 1.12\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"gcsfs\",\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def bias_detect_component(\n",
    "    code_bucket_path: str,\n",
    "    metrics: Input[Metrics],\n",
    "    gcs_artifact_path: str,\n",
    ")-> NamedTuple(\"output\", [(\"bias_detect\", str)]):\n",
    "    import logging\n",
    "    import json\n",
    "    import importlib.util\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "\n",
    "    # Logging setup\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Download code from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(code_bucket_path.split('/')[2])\n",
    "    prefix = '/'.join(code_bucket_path.split('/')[3:])\n",
    "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
    "\n",
    "    code_dir = \"/tmp/code\"\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    ALLOWED_EXTENSIONS = {\".py\", \".json\", \".yaml\", \".csv\", \".pkl\"}\n",
    "\n",
    "    for blob in blobs:\n",
    "        if any(blob.name.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
    "            relative_path = blob.name[len(prefix):].lstrip(\"/\")\n",
    "            file_path = os.path.join(code_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            blob.download_to_filename(file_path)\n",
    "            logger.info(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    logger.info(f\"Files in {code_dir}: {os.listdir(code_dir)}\")\n",
    "    sys.path.insert(0, code_dir)\n",
    "\n",
    "    def load_module_from_file(file_path):\n",
    "        module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    # Ensure `evaluate_module_slices.py` exists\n",
    "    evaluate_script_path = os.path.join(code_dir, \"bias_detect.py\")\n",
    "    if not os.path.exists(evaluate_script_path):\n",
    "        raise FileNotFoundError(f\"`evaluate_module_slices.py` not found in {code_dir}\")\n",
    "\n",
    "    # Load `evaluate_module_slices.py` dynamically\n",
    "    bias_detect = load_module_from_file(evaluate_script_path)\n",
    "\n",
    "\n",
    "    # Call `gcp_eval` method from the module\n",
    "    biased_rows, f1_threshold = bias_detect.detect_bias(\n",
    "        slice_metrics_path=metrics.metadata[\"slice_metrics_csv\"],\n",
    "    )\n",
    "    bias_report = {}\n",
    "\n",
    "    # Log results\n",
    "    if not biased_rows.empty:\n",
    "        bias_report[\"bias_detected\"] = True\n",
    "        bias_report[\"details\"] = biased_rows.to_dict(orient=\"records\")\n",
    "\n",
    "        logger.warning(\"Potential bias detected in the following slices:\")\n",
    "        for _, row in biased_rows.iterrows():\n",
    "            logger.error(\n",
    "                f\"Slice Column: {row['Slice Column']}, Slice Value: {row['Slice Value']}, \"\n",
    "                f\"Samples: {row['Samples']}, F1 Score: {row['F1 Score']:.4f} (Threshold: {f1_threshold:.4f})\"\n",
    "            )\n",
    "        logger.error(\"Potential bias detected. Check bias_detection.log for details.\")\n",
    "    else:\n",
    "        bias_report[\"bias_detected\"] = False\n",
    "        bias_report[\"details\"] = []\n",
    "        logger.info(\"No significant bias detected.\")\n",
    "        # print(\"No significant bias detected.\")\n",
    "\n",
    "    # Save bias report as JSON to GCS\n",
    "    gcs_bucket_name = gcs_artifact_path.split('/')[2]\n",
    "    gcs_blob_path = '/'.join(gcs_artifact_path.split('/')[3:])\n",
    "    bias_json_path = f\"{gcs_blob_path}/bias.json\"\n",
    "\n",
    "    try:\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(gcs_bucket_name)\n",
    "        blob = bucket.blob(bias_json_path)\n",
    "        blob.upload_from_string(json.dumps(bias_report, indent=4), content_type=\"application/json\")\n",
    "        logging.info(f\"Bias report saved to GCS at gs://{gcs_bucket_name}/{bias_json_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save bias report to GCS: {e}\")\n",
    "        raise\n",
    "\n",
    "    # bias_metrics.log_metric(\"bias_detected\",)\n",
    "    \n",
    "    # Raise an error if bias is detected to stop the pipeline\n",
    "    if bias_report[\"bias_detected\"]:\n",
    "        # bias_metrics.log_metric(\"bias_detected\",True)\n",
    "        bias_detect = \"true\"\n",
    "        return (bias_detect,)\n",
    "        # raise RuntimeError(\"Bias detected in slice metrics. Stopping the pipeline.\")\n",
    "    else:\n",
    "        bias_detect = \"false\"\n",
    "        return (bias_detect,)\n",
    "        # bias_metrics.log_metric(\"bias_detected\",False)\n",
    "\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage\", \"google-cloud-build\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def build_and_push_torchserve_image(\n",
    "    code_bucket_path: str,  # This is the missing input parameter\n",
    "    gcp_project: str, \n",
    "    gcp_region: str, \n",
    "    bucket_name: str, \n",
    "    docker_image_name: str,\n",
    "    model_gcs_path: Input[Model]\n",
    "):\n",
    "    # Import inside the component\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import os\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Define environment variables\n",
    "    TORCH_SERVE_PATH = f\"gs://{bucket_name}/code/predictor/\"\n",
    "    CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{gcp_project}/{docker_image_name}\"\n",
    "\n",
    "    # Set up the CloudBuild client\n",
    "    client = cloudbuild.CloudBuildClient()\n",
    "\n",
    "    # Log the environment variables for debugging\n",
    "    logger.info(f\"GCP Project: {gcp_project}\")\n",
    "    logger.info(f\"GCP Region: {gcp_region}\")\n",
    "    logger.info(f\"Bucket Name: {bucket_name}\")\n",
    "    logger.info(f\"TorchServe Path: {TORCH_SERVE_PATH}\")\n",
    "    logger.info(f\"Docker Image Name: {docker_image_name}\")\n",
    "    logger.info(f\"Custom Docker Image URI: {CUSTOM_PREDICTOR_IMAGE_URI}\")\n",
    "        \n",
    "    model_gcs_path = model_gcs_path.metadata[\"gcs_path\"]\n",
    "    model_gcs_path = f\"gs://arsa_model_deployment_uscentral_v2/output/models/\"\n",
    "    # Create Cloud Build configuration (cloudbuild.yaml)\n",
    "    cloudbuild_config = {\n",
    "        'steps': [\n",
    "            # Step 1: Download code files from GCS\n",
    "            {\n",
    "                \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "                \"args\": [\n",
    "                    'cp',\n",
    "                    '-r',  # Recursive copy\n",
    "                    f'{code_bucket_path}/*',  # Copy all contents from the code folder\n",
    "                    '.'  # Copy to the current working directory\n",
    "                ],\n",
    "            },\n",
    "            # Step 2: Create the destination directory for model files\n",
    "            {\n",
    "                \"name\": \"ubuntu\",\n",
    "                \"args\": [\n",
    "                    \"mkdir\",\n",
    "                    \"-p\",  # Create parent directories as needed\n",
    "                    \"./bert-sent-model\"\n",
    "                ],\n",
    "            },\n",
    "            # Step 3: Download model files from GCS\n",
    "            {\n",
    "                \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "                \"args\": [\n",
    "                    'cp',\n",
    "                    '-r',  # Recursive copy\n",
    "                    f'{model_gcs_path}*',  # Add wildcard to include all files in the folder\n",
    "                    './bert-sent-model/'  # Ensure the trailing slash\n",
    "                ],\n",
    "            },\n",
    "            # Step 3: List files in the current working directory\n",
    "            {\n",
    "                \"name\": \"ubuntu\",\n",
    "                \"args\": [\n",
    "                    \"ls\",\n",
    "                    \"-R\",  # Recursive listing\n",
    "                    \".\"    # Current working directory\n",
    "                ],\n",
    "            },\n",
    "            # Step 4: Build the Docker image\n",
    "            {\n",
    "                'name': 'gcr.io/cloud-builders/docker',\n",
    "                'args': [\n",
    "                    'build',\n",
    "                    '-t',\n",
    "                    CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "                    '.'\n",
    "                ],\n",
    "            },\n",
    "            # Step 5: Push the Docker image to the container registry\n",
    "            {\n",
    "                'name': 'gcr.io/cloud-builders/docker',\n",
    "                'args': [\n",
    "                    'push',\n",
    "                    CUSTOM_PREDICTOR_IMAGE_URI\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        'images': [CUSTOM_PREDICTOR_IMAGE_URI],\n",
    "    }\n",
    "\n",
    "    # Create a Cloud Build build request\n",
    "    build = cloudbuild.Build(\n",
    "        steps=cloudbuild_config['steps'],\n",
    "        images=cloudbuild_config['images'],\n",
    "    )\n",
    "\n",
    "    # Trigger Cloud Build job\n",
    "    build_response = client.create_build(project_id=gcp_project, build=build)\n",
    "\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(build_response.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = build_response.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"google-auth\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def upload_model_to_registry(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    model_display_name: str,\n",
    "    docker_image_uri: str,\n",
    "    model_description: str,\n",
    "    app_name: str,\n",
    "    health_route: str = \"/ping\",\n",
    "    predict_route: str = \"/predictions/\",\n",
    "    serving_container_ports: list = [7080],\n",
    ") -> NamedTuple(\"Outputs\", [(\"model_display_name\", str), (\"model_resource_name\", str), (\"model_version\", str)]):\n",
    "    \"\"\"Uploads the model to the AI platform and ensures versioning.\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    # Initialize the AI Platform\n",
    "    aiplatform.init(project=project_id, location=region, staging_bucket=bucket_name)\n",
    "\n",
    "    # Check if the model with the same display name exists\n",
    "    existing_models = aiplatform.Model.list(filter=f\"display_name={model_display_name}\")\n",
    "    \n",
    "    if existing_models:\n",
    "        # Model exists, register as a new version\n",
    "        model_resource_name = existing_models[0].resource_name\n",
    "        print(f\"Model with display name '{model_display_name}' exists. Registering as a new version.\")\n",
    "        model_version = f\"v{len(existing_models) + 1}\"  # Increment version number\n",
    "    else:\n",
    "        # Model does not exist, create a new one\n",
    "        model_resource_name = None\n",
    "        print(f\"Model with display name '{model_display_name}' does not exist. Creating a new model.\")\n",
    "        model_version = \"v1\"\n",
    "\n",
    "    # Upload the model\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        description=model_description,\n",
    "        serving_container_image_uri=docker_image_uri,\n",
    "        serving_container_predict_route=predict_route + app_name,\n",
    "        serving_container_health_route=health_route,\n",
    "        serving_container_ports=serving_container_ports,\n",
    "        parent_model=model_resource_name,  # Register under an existing model if applicable\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    # Return output information\n",
    "    return (model.display_name, model.resource_name, model_version)\n",
    "\n",
    "\n",
    "# @dsl.pipeline(\n",
    "#     name=\"data-prep-and-train\",\n",
    "#     pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    "# )\n",
    "# def data_prep_and_train_pipeline():\n",
    "#     # Step 1: Data Preparation\n",
    "#     data_prep_task = data_prep_stage(\n",
    "#         code_bucket_path=SOURCE_CODE,\n",
    "#         input_path=DATA_PATH,\n",
    "#         output_dir=OUTPUT_DIR,\n",
    "#     )\n",
    "\n",
    "#     # Step 2: Training and Saving Model\n",
    "#     train_save_task = train_save_stage(\n",
    "#         code_bucket_path=SOURCE_CODE,\n",
    "#         data_path=OUTPUT_DIR,\n",
    "#         model_save_path=MODEL_SAVE_PATH,\n",
    "#         train_data=data_prep_task.outputs[\"train_data\"],\n",
    "#         val_data=data_prep_task.outputs[\"val_data\"],\n",
    "\n",
    "#     ).set_cpu_limit(\"8\") \\\n",
    "#      .set_memory_limit(\"32G\") \\\n",
    "#      .set_gpu_limit(1) \\\n",
    "#      .set_accelerator_type(\"NVIDIA_TESLA_T4\")\n",
    "\n",
    "#     evaluate_task = evaluate_model_component(\n",
    "#         code_bucket_path=SOURCE_CODE,\n",
    "#         model_gcs_path=train_save_task.outputs[\"model\"],  # Pass Model artifact\n",
    "#         test_data=data_prep_task.outputs[\"test_data\"],  # Pass Test Data artifact\n",
    "#         f1_threshold=0.6,\n",
    "#     )\n",
    "    \n",
    "    # evaluate_slices_task = evaluate_slices_component(\n",
    "    #     code_bucket_path=SOURCE_CODE,\n",
    "    #     model_gcs_path=train_save_task.outputs[\"model\"], \n",
    "    #     test_data=data_prep_task.outputs[\"test_data\"],  \n",
    "    #     gcs_artifact_path = SLICE_METRIC_PATH,\n",
    "    #     f1_threshold=0.6,\n",
    "    # )\n",
    "    # bias_detect_task = bias_detect_component(\n",
    "    #     code_bucket_path=SOURCE_CODE,\n",
    "    #     metrics=evaluate_slices_task.outputs[\"eval_slices_metrics\"],\n",
    "    #     gcs_artifact_path = SLICE_METRIC_PATH,\n",
    "    # )\n",
    "\n",
    "    # build_and_push_torchserve_image_op = build_and_push_torchserve_image(\n",
    "    #         code_bucket_path=SOURCE_CODE, \n",
    "    #         gcp_project=GCP_PROJECT,\n",
    "    #         gcp_region=GCP_REGION,\n",
    "    #         bucket_name=BUCKET_NAME,\n",
    "    #         docker_image_name=\"pytorch_predict_review_sentiment_bert_model\",\n",
    "    #         model_gcs_path=train_save_task.outputs[\"model\"],\n",
    "    #     )\n",
    "    \n",
    "    # train_save_task.after(data_prep_task)\n",
    "    # evaluate_task.after(train_save_task)\n",
    "\n",
    "    # evaluate_slices_task.after(evaluate_task)  \n",
    "    # bias_detect_task.after(evaluate_slices_task)\n",
    "    # build_and_push_torchserve_image_op.after(bias_detect_task)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"data-prep-and-train\",\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    ")\n",
    "def data_prep_and_train_pipeline():\n",
    "    # Step 1: Data Preparation\n",
    "    data_prep_task = data_prep_stage(\n",
    "        code_bucket_path=SOURCE_CODE,\n",
    "        input_path=DATA_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "    )\n",
    "\n",
    "    # Step 2: Training and Saving Model\n",
    "    train_save_task = train_save_stage(\n",
    "        code_bucket_path=SOURCE_CODE,\n",
    "        data_path=OUTPUT_DIR,\n",
    "        model_save_path=MODEL_SAVE_PATH,\n",
    "        train_data=data_prep_task.outputs[\"train_data\"],\n",
    "        val_data=data_prep_task.outputs[\"val_data\"],\n",
    "    ).set_cpu_limit(\"8\") \\\n",
    "     .set_memory_limit(\"32G\") \\\n",
    "     .set_gpu_limit(1) \\\n",
    "     .set_accelerator_type(\"NVIDIA_TESLA_T4\")\n",
    "\n",
    "    # Step 3: Model Evaluation\n",
    "    evaluate_task = evaluate_model_component(\n",
    "        code_bucket_path=SOURCE_CODE,\n",
    "        model_gcs_path=train_save_task.outputs[\"model\"],  # Pass Model artifact\n",
    "        test_data=data_prep_task.outputs[\"test_data\"],  # Pass Test Data artifact\n",
    "        f1_threshold=0.6,\n",
    "    )\n",
    "\n",
    "    # Conditional Logic: Check if eval passed\n",
    "    with dsl.If(evaluate_task.outputs[\"eval_pass\"] == \"true\", name=\"conditional-validation-check\"):\n",
    "        # Step 4: Evaluate Slices\n",
    "        evaluate_slices_task = evaluate_slices_component(\n",
    "            code_bucket_path=SOURCE_CODE,\n",
    "            model_gcs_path=train_save_task.outputs[\"model\"], \n",
    "            test_data=data_prep_task.outputs[\"test_data\"],  \n",
    "            gcs_artifact_path=SLICE_METRIC_PATH,\n",
    "            f1_threshold=0.6,\n",
    "        )\n",
    "        \n",
    "        # Step 5: Bias Detection\n",
    "        bias_detect_task = bias_detect_component(\n",
    "            code_bucket_path=SOURCE_CODE,\n",
    "            metrics=evaluate_slices_task.outputs[\"eval_slices_metrics\"],\n",
    "            gcs_artifact_path=SLICE_METRIC_PATH,\n",
    "        )\n",
    "        evaluate_slices_task.after(evaluate_task)\n",
    "        bias_detect_task.after(evaluate_slices_task)\n",
    "\n",
    "        with dsl.If(bias_detect_task.outputs[\"bias_detect\"] == \"false\", name=\"bias-check-condtional-deploy\"):\n",
    "\n",
    "            # Step 6: Build and Push TorchServe Image\n",
    "            build_and_push_torchserve_image_op = build_and_push_torchserve_image(\n",
    "                code_bucket_path=SOURCE_CODE, \n",
    "                gcp_project=GCP_PROJECT,\n",
    "                gcp_region=GCP_REGION,\n",
    "                bucket_name=BUCKET_NAME,\n",
    "                docker_image_name=\"pytorch_predict_review_sentiment_bert_model\",\n",
    "                model_gcs_path=train_save_task.outputs[\"model\"],\n",
    "            )\n",
    "        # Task dependencies within the successful branch\n",
    "\n",
    "\n",
    "            upload_model_task = upload_model_to_registry(\n",
    "                project_id=PROJECT_ID,\n",
    "                region=GCP_REGION,\n",
    "                bucket_name=BUCKET_NAME,\n",
    "                model_display_name=MODEL_DISPLAY_NAME,\n",
    "                docker_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "                model_description=MODEL_DESCRIPTION,\n",
    "                app_name=APP_NAME,\n",
    "            )\n",
    "            build_and_push_torchserve_image_op.after(bias_detect_task)\n",
    "            upload_model_task.after(build_and_push_torchserve_image_op)\n",
    "\n",
    "\n",
    "    # Loop Back: If F1 < 0.6\n",
    "    # with dsl.If(evaluate_task.outputs[\"f1_score\"]  <= 0.6):\n",
    "    #     train_save_task.after(evaluate_task)\n",
    "\n",
    "from kfp.v2.compiler import Compiler\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Define the pipeline file path\n",
    "pipeline_file_path = \"data_prep_and_train_pipeline.json\"\n",
    "\n",
    "# Compile the pipeline\n",
    "Compiler().compile(pipeline_func=data_prep_and_train_pipeline, package_path=pipeline_file_path)\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=GCP_PROJECT, location=GCP_REGION)\n",
    "\n",
    "# Submit the pipeline to Vertex AI\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"data-prep-and-train-pipeline\",\n",
    "    template_path=pipeline_file_path,\n",
    "    pipeline_root=f\"gs://{BUCKET_NAME}/pipeline_root/\",\n",
    ")\n",
    "\n",
    "pipeline_job.submit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kfp\n",
    "# from kfp.v2.dsl import pipeline, component\n",
    "# from kfp.v2 import compiler\n",
    "# from google.cloud import storage\n",
    "# import os\n",
    "\n",
    "# # Define the pipeline component\n",
    "# @component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-storage\"])\n",
    "# def gcs_transfer_component(\n",
    "#     source_bucket: str, destination_bucket: str, file_path_source: str, file_path_dest: str\n",
    "# ):\n",
    "#     \"\"\"Component to transfer files between GCS buckets.\"\"\"\n",
    "#     from google.cloud import storage  # Import inside component to avoid issues\n",
    "#     import os\n",
    "#     # Initialize GCS client\n",
    "#     storage_client = storage.Client()\n",
    "\n",
    "#     # Download the file from the source bucket\n",
    "#     source_blob = storage_client.bucket(source_bucket).blob(file_path_source)\n",
    "#     local_file = f\"/tmp/{os.path.basename(file_path_source)}\"\n",
    "#     source_blob.download_to_filename(local_file)\n",
    "#     print(f\"Downloaded {file_path_source} from bucket {source_bucket} to {local_file}\")\n",
    "\n",
    "#     # Upload the file to the destination bucket\n",
    "#     dest_blob = storage_client.bucket(destination_bucket).blob(f\"{file_path_dest}/{os.path.basename(file_path_source)}\")\n",
    "#     dest_blob.upload_from_filename(local_file)\n",
    "#     print(f\"Uploaded {file_path_source} to {destination_bucket}/{file_path_dest}\")\n",
    "\n",
    "# # Define the pipeline\n",
    "# @pipeline(name=\"gcs-transfer-pipeline\")\n",
    "# def gcs_pipeline(\n",
    "#     source_bucket: str = \"arsa_model_deployment_uscentral\",\n",
    "#     destination_bucket: str = \"arsa_model_deployment_uscentral_v2\",\n",
    "#     file_path_source: str = \"code/predictor/bert-sent-model/final_model.pth\",\n",
    "#     file_path_dest: str = \"output/models\",\n",
    "# ):\n",
    "#     # Add the GCS transfer component to the pipeline\n",
    "#     gcs_transfer_component(\n",
    "#         source_bucket=source_bucket,\n",
    "#         destination_bucket=destination_bucket,\n",
    "#         file_path_source=file_path_source,\n",
    "#         file_path_dest=file_path_dest,\n",
    "#     )\n",
    "\n",
    "# # Compile and run the pipeline from a notebook\n",
    "# from google.cloud import aiplatform\n",
    "\n",
    "# # Set GCP and Vertex AI configuration\n",
    "# PROJECT_ID = \"amazonreviewssentimentanalysis\"\n",
    "# REGION = \"us-central1\"\n",
    "# PIPELINE_NAME = \"gcs-transfer-pipeline\"\n",
    "# STAGING_BUCKET = \"gs://arsa_model_deployment_uscentral\"  # Fully qualified GCS bucket URI\n",
    "\n",
    "# # Compile the pipeline\n",
    "# pipeline_json = f\"{PIPELINE_NAME}.json\"\n",
    "# compiler.Compiler().compile(\n",
    "#     pipeline_func=gcs_pipeline,\n",
    "#     package_path=pipeline_json,\n",
    "# )\n",
    "\n",
    "# # Initialize the Vertex AI client\n",
    "# aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# # Submit the pipeline job\n",
    "# pipeline_job = aiplatform.PipelineJob(\n",
    "#     display_name=PIPELINE_NAME,\n",
    "#     template_path=pipeline_json,\n",
    "#     parameter_values={\n",
    "#         \"source_bucket\": \"arsa_model_deployment_uscentral\",\n",
    "#         \"destination_bucket\": \"arsa_model_deployment_uscentral_v2\",\n",
    "#         \"file_path_source\": \"code/predictor/bert-sent-model/final_model.pth\",\n",
    "#         \"file_path_dest\": \"output/models\",\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # Run the pipeline\n",
    "# pipeline_job.submit()\n",
    "\n",
    "# print(f\"Pipeline {PIPELINE_NAME} has been submitted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
